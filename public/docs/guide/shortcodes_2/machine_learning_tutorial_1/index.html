<!doctype html><!-- This site was created with Hugo Blox. https://hugoblox.com --><!-- Last Published: March 16, 2025 --><html lang=en-us dir=ltr data-wc-theme-default=dark><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=56235&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 0.2.0"><meta name=author content="Mohammad Haidar"><meta name=description content="Introduction Welcome to this introductory course on Machine Learning (ML), where we will explore the fundamental concepts and techniques that are the building blocks for more advanced fields, such as Quantum Machine Learning."><link rel=alternate hreflang=en-us href=http://localhost:56235/docs/guide/shortcodes_2/machine_learning_tutorial_1/><link rel=stylesheet href=/css/themes/green.min.css><link href=/dist/wc.min.css rel=stylesheet><script>window.hbb={defaultTheme:document.documentElement.dataset.wcThemeDefault,setDarkTheme:()=>{document.documentElement.classList.add("dark"),document.documentElement.style.colorScheme="dark"},setLightTheme:()=>{document.documentElement.classList.remove("dark"),document.documentElement.style.colorScheme="light"}},console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`),"wc-color-theme"in localStorage?localStorage.getItem("wc-color-theme")==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme():(window.hbb.defaultTheme==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme(),window.hbb.defaultTheme==="system"&&(window.matchMedia("(prefers-color-scheme: dark)").matches?window.hbb.setDarkTheme():window.hbb.setLightTheme()))</script><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll("li input[type='checkbox'][disabled]");e.forEach(e=>{e.parentElement.parentElement.classList.add("task-list")});const t=document.querySelectorAll(".task-list li");t.forEach(e=>{let t=Array.from(e.childNodes).filter(e=>e.nodeType===3&&e.textContent.trim().length>1);if(t.length>0){const n=document.createElement("label");t[0].after(n),n.appendChild(e.querySelector("input[type='checkbox']")),n.appendChild(t[0])}})})</script><link rel=icon type=image/png href=/media/icon_hu71dbac049849e17038d3cfeefdabc10a_16663_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu71dbac049849e17038d3cfeefdabc10a_16663_180x180_fill_lanczos_center_3.png><link rel=canonical href=http://localhost:56235/docs/guide/shortcodes_2/machine_learning_tutorial_1/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@GetResearchDev"><meta property="twitter:creator" content="@GetResearchDev"><meta property="og:site_name" content="OpenVQE"><meta property="og:url" content="http://localhost:56235/docs/guide/shortcodes_2/machine_learning_tutorial_1/"><meta property="og:title" content="Classical Machine Learning Tutorial 1 | OpenVQE"><meta property="og:description" content="Introduction Welcome to this introductory course on Machine Learning (ML), where we will explore the fundamental concepts and techniques that are the building blocks for more advanced fields, such as Quantum Machine Learning."><meta property="og:image" content="http://localhost:56235/media/logo.svg"><meta property="twitter:image" content="http://localhost:56235/media/logo.svg"><meta property="og:locale" content="en-us"><title>Classical Machine Learning Tutorial 1 | OpenVQE</title><style>@font-face{font-family:inter var;font-style:normal;font-weight:100 900;font-display:swap;src:url(/dist/font/Inter.var.woff2)format(woff2)}</style><link type=text/css rel=stylesheet href=/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css integrity="sha256-vnZutBkxehTsdp0hbpd5v+jzc3yA54D0ug2vtXpBpII="><script src=/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script><script>window.hbb.pagefind={baseUrl:"/"}</script><style>html.dark{--pagefind-ui-primary:#eeeeee;--pagefind-ui-text:#eeeeee;--pagefind-ui-background:#152028;--pagefind-ui-border:#152028;--pagefind-ui-tag:#152028}</style><script>window.addEventListener("DOMContentLoaded",e=>{new PagefindUI({element:"#search",showSubResults:!0,baseUrl:window.hbb.pagefind.baseUrl,bundlePath:window.hbb.pagefind.baseUrl+"pagefind/"})}),document.addEventListener("DOMContentLoaded",()=>{let e=document.getElementById("search"),t=document.getElementById("search_toggle");t&&t.addEventListener("click",()=>{if(e.classList.toggle("hidden"),e.querySelector("input").value="",e.querySelector("input").focus(),!e.classList.contains("hidden")){let t=document.querySelector(".pagefind-ui__search-clear");t&&!t.hasAttribute("listenerOnClick")&&(t.setAttribute("listenerOnClick","true"),t.addEventListener("click",()=>{e.classList.toggle("hidden")}))}})})</script><link type=text/css rel=stylesheet href=/dist/lib/katex/katex.min.505d5f829022bb7b4f24dfee0aa1141cd7bba67afe411d1240335f820960b5c3.css integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM="><script defer src=/dist/lib/katex/katex.min.dc84b296ec3e884de093158f760fd9d45b6c7abe58b5381557f4e138f46a58ae.js integrity="sha256-3ISyluw+iE3gkxWPdg/Z1Ftser5YtTgVV/ThOPRqWK4="></script><script defer src=/js/katex-renderer.6579ec9683211cfb952064aedf3a3baea5eeb17a061775b32b70917474637c80.js integrity="sha256-ZXnsloMhHPuVIGSu3zo7rqXusXoGF3WzK3CRdHRjfIA="></script><script defer src=/js/hugo-blox-en.min.js integrity></script></head><body class="dark:bg-hb-dark dark:text-white page-wrapper" id=top><div id=page-bg></div><div class="page-header sticky top-0 z-30"><header id=site-header class=header><nav class="navbar px-3 flex justify-left"><div class="order-0 h-100"><a class=navbar-brand href=/ title=OpenVQE><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="500" zoomAndPan="magnify" viewBox="0 0 375 374.999991" height="500" version="1.0"><defs><g/><clipPath id="e3641af915"><path d="M34.742188 221.066406H187.5v4.007813H34.742188zm0 0" clip-rule="nonzero"/></clipPath></defs><rect x="-37.5" width="450" fill="#fff" y="-37.499999" height="449.999989" fill-opacity="1"/><rect x="-37.5" width="450" fill="#ebebeb" y="-37.499999" height="449.999989" fill-opacity="1"/><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(7.589851, 210.355429)"><g><path d="M60.375-14.609375C57.632812-9.921875 53.984375-6.238281 49.421875-3.5625 44.785156-.820312 39.757812.546875 34.34375.546875c-5.480469.0-10.5-1.367187-15.0625-4.109375C14.644531-6.238281 10.988281-9.921875 8.3125-14.609375 5.632812-19.242188 4.296875-24.300781 4.296875-29.78125c0-5.414062 1.335937-10.46875 4.015625-15.15625C11.050781-49.570312 14.734375-53.257812 19.359375-56c4.570313-2.675781 9.566406-4.015625 14.984375-4.015625 5.363281.0 10.390625 1.339844 15.078125 4.015625 4.625 2.742188 8.273437 6.429688 10.953125 11.0625 2.675781 4.6875 4.015625 9.742188 4.015625 15.15625.0 5.417969-1.339844 10.476562-4.015625 15.171875zm-6.578125-26.5c-2.011719-3.46875-4.75-6.238281-8.21875-8.3125-3.46875-2.007813-7.214844-3.015625-11.234375-3.015625-4.085938.0-7.828125 1.007812-11.234375 3.015625-3.46875 2.074219-6.210937 4.84375-8.21875 8.3125C12.878906-37.628906 11.875-33.851562 11.875-29.78125c0 4.148438 1.003906 7.953125 3.015625 11.421875 2.007813 3.46875 4.75 6.210937 8.21875 8.21875C26.578125-8.066406 30.320312-7.03125 34.34375-7.03125c4.144531.0 7.921875-1.035156 11.328125-3.109375 3.46875-2.007813 6.207031-4.75 8.21875-8.21875 1.945313-3.46875 2.921875-7.273437 2.921875-11.421875.0-4.070312-1.007812-7.847656-3.015625-11.328125zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(68.336812, 210.355429)"><g><path d="M7.765625.0V-59.46875H29.5c2.625.0 5.148438.429687999999999 7.578125 1.28125 2.375.855468999999999 4.476563 2.074219 6.3125 3.65625 1.945313 1.648438 3.46875 3.5625 4.5625 5.75 1.039063 2.199219 1.5625 4.484375 1.5625 6.859375.0 2.4375-.523437000000001 4.71875-1.5625 6.84375-1.09375 2.199219-2.617187 4.148437-4.5625 5.84375-1.835937 1.585937-3.9375 2.804687-6.3125 3.65625C34.648438-24.660156 32.125-24.203125 29.5-24.203125H15.53125V0zM15.53125-31.875H29.5c2.257812.0 4.332031-.457031000000001 6.21875-1.375C37.601562-34.164062 39.125-35.382812 40.28125-36.90625c1.09375-1.519531 1.640625-3.191406 1.640625-5.015625.0-1.769531-.546875-3.414063-1.640625-4.9375-1.15625-1.519531-2.679688-2.738281-4.5625-3.65625C33.769531-51.367188 31.695312-51.796875 29.5-51.796875H15.34375zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(115.107322, 210.355429)"><g><path d="M7.765625-59.46875h38v7.671875H15.34375v18.1875H38.640625V-25.9375H15.34375V-7.578125H45.765625V0h-38zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(160.690288, 210.355429)"><g><path d="M15.53125.0H7.859375V-60.375L46.5-18.640625V-59.46875h7.578125V1.46875L15.53125-40.46875zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(214.586049, 210.355429)"><g><path d="M12.328125-59.46875l18.8125 42.109375L49.96875-59.46875H58.375L31.140625 1.46875l-27.125-60.9375zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(269.212591, 210.355429)"><g><path d="M55.71875.0 51.25-4.65625C46.070312-1.1875 40.4375.546875 34.34375.546875c-5.417969.0-10.414062-1.367187-14.984375-4.109375-4.625-2.675781-8.308594-6.359375-11.046875-11.046875-2.679688-4.695313-4.015625-9.753906-4.015625-15.171875.0-5.414062 1.335937-10.46875 4.015625-15.15625C10.988281-49.570312 14.644531-53.257812 19.28125-56c4.625-2.675781 9.644531-4.015625 15.0625-4.015625 5.363281.0 10.359375 1.339844 14.984375 4.015625 4.625 2.742188 8.304687 6.429688 11.046875 11.0625 2.675781 4.6875 4.015625 9.742188 4.015625 15.15625C64.390625-26 63.753906-22.40625 62.484375-19c-1.34375 3.46875-3.203125 6.574219-5.578125 9.3125L66.125.0zM45.859375-10.328125 30.59375-26.765625 36.171875-31.875l15.53125 16.53125c3.40625-4.144531 5.109375-8.957031 5.109375-14.4375.0-4.019531-.976562000000001-7.796875-2.921875-11.328125-2.011719-3.46875-4.75-6.238281-8.21875-8.3125-3.46875-2.007813-7.246094-3.015625-11.328125-3.015625-4.023438.0-7.765625 1.007812-11.234375 3.015625-3.46875 2.074219-6.210937 4.84375-8.21875 8.3125C12.878906-37.628906 11.875-33.851562 11.875-29.78125c0 4.085938 1.003906 7.859375 3.015625 11.328125 2.007813 3.46875 4.75 6.210937 8.21875 8.21875C26.460938-8.160156 30.207031-7.125 34.34375-7.125c4.257812.0 8.097656-1.066406 11.515625-3.203125zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(332.151927, 210.355429)"><g><path d="M7.765625-59.46875h38v7.671875H15.34375v18.1875H38.640625V-25.9375H15.34375V-7.578125H45.765625V0h-38zm0 0"/></g></g></g><g clip-path="url(#e3641af915)"><path fill="#2e2e2e" d="M34.742188 221.066406H187.5v4.007813H34.742188zm0 0" fill-opacity="1" fill-rule="nonzero"/></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(193.251425, 226.635395)"><g><path d="M3.34375.109375c-.585938.0-1.105469-.125-1.5625-.375-.449219-.25-.804688-.59375-1.0625-1.03125-.25-.4375-.375-.941406-.375-1.515625.0-.570312.125-1.078125.375-1.515625C.976562-4.773438 1.332031-5.125 1.78125-5.375c.457031-.25.976562-.375 1.5625-.375.59375.0 1.113281.125 1.5625.375.445312.25.800781.601562 1.0625 1.046875.257812.4375.390625.945313.390625 1.515625.0.617188-.152344 1.15625-.453125 1.625-.304688.46875-.710938.8125-1.21875 1.03125L4.75.0c.0625.125.117188.21875.171875.28125.0625.070312.144531.117188.25.140625.101563.03125.242187.046875.421875.046875h.4375v1H5.3125c-.8125.0-1.367188-.351562-1.65625-1.046875l-.125-.3125zM1.65625-2.8125c0 .335938.066406.632812.203125.890625.144531.261719.34375.464844.59375.609375.257813.136719.554687.203125.890625.203125.34375.0.640625-.066406.890625-.203125.257813-.144531.457031-.347656.59375-.609375.144531-.257813.21875-.554687.21875-.890625.0-.34375-.0742190000000003-.640625-.21875-.890625-.136719-.257813-.335937-.460937-.59375-.609375-.25-.144531-.546875-.21875-.890625-.21875-.335938.0-.632812.0742190000000003-.890625.21875-.25.148438-.449219.351562-.59375.609375-.136719.25-.203125.546875-.203125.890625zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(201.90442, 226.635395)"><g><path d="M2.984375.109375c-.492187.0-.914063-.09375-1.265625-.28125-.355469-.195313-.625-.476563-.8125-.84375C.71875-1.378906.625-1.8125.625-2.3125V-5.625H1.90625v3.21875c0 .867188.359375 1.296875 1.078125 1.296875S4.0625-1.539062 4.0625-2.40625V-5.625H5.328125v3.3125c0 .5-.09375.933594-.28125 1.296875-.1875.367187-.460937.648437-.8125.84375-.355469.1875-.773437.28125-1.25.28125zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(209.809532, 226.635395)"><g><path d="M.078125.0 2.3125-5.625h1.25L5.796875.0H4.4375l-.375-.96875h-2.25L1.4375.0zM2.15625-1.984375H3.734375L2.9375-4.046875zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(217.634218, 226.635395)"><g><path d="M.671875.0V-5.625h1.09375L4.25-2.203125V-5.625H5.515625V0H4.4375L1.953125-3.453125V0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(225.780584, 226.635395)"><g><path d="M2.03125.0V-4.4375H.234375V-5.625H5.09375v1.1875H3.3125V0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(233.066471, 226.635395)"><g><path d="M2.984375.109375c-.492187.0-.914063-.09375-1.265625-.28125-.355469-.195313-.625-.476563-.8125-.84375C.71875-1.378906.625-1.8125.625-2.3125V-5.625H1.90625v3.21875c0 .867188.359375 1.296875 1.078125 1.296875S4.0625-1.539062 4.0625-2.40625V-5.625H5.328125v3.3125c0 .5-.09375.933594-.28125 1.296875-.1875.367187-.460937.648437-.8125.84375-.355469.1875-.773437.28125-1.25.28125zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(240.971583, 226.635395)"><g><path d="M.671875.0V-5.625h1.09375l2 2.90625 2-2.90625H6.84375V0H5.5625V-3.390625l-1.359375 2H3.3125l-1.359375-2V0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(250.436811, 226.635395)"><g/></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(254.280796, 226.635395)"><g><path d="M3.265625.109375c-.574219.0-1.078125-.125-1.515625-.375S.96875-.859375.71875-1.296875.34375-2.238281.34375-2.8125c0-.570312.125-1.078125.375-1.515625.25-.445313.585938-.796875 1.015625-1.046875.4375-.25.941406-.375 1.515625-.375.476562.0.914062.09375 1.3125.28125.394531.179688.726562.433594 1 .765625.269531.335937.457031.726563.5625 1.171875H4.8125c-.105469-.300781-.296875-.539062-.578125-.71875-.273437-.1875-.585937-.28125-.9375-.28125C2.972656-4.53125 2.6875-4.457031 2.4375-4.3125c-.25.148438-.445312.351562-.578125.609375-.136719.25-.203125.546875-.203125.890625.0.335938.066406.632812.203125.890625.132813.261719.328125.464844.578125.609375.25.136719.53125.203125.84375.203125.375.0.695312-.09375.96875-.28125.28125-.1875.484375-.441406.609375-.765625h1.3125c-.09375.460938-.277344.859375-.546875 1.203125-.273438.335937-.609375.59375-1.015625.78125s-.855469.28125-1.34375.28125zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(262.724707, 226.635395)"><g><path d="M4.15625.0V-2.265625H1.953125V0H.671875V-5.625h1.28125v2.15625H4.15625V-5.625H5.4375V0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(270.790658, 226.635395)"><g><path d="M.671875.0V-5.625H4.375v1.078125H1.953125v1.15625H4.15625V-2.3125H1.953125v1.21875H4.46875V0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(277.61816, 226.635395)"><g><path d="M.671875.0V-5.625h1.09375l2 2.90625 2-2.90625H6.84375V0H5.5625V-3.390625l-1.359375 2H3.3125l-1.359375-2V0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(287.083389, 226.635395)"><g><path d="M.671875.0V-5.625h1.28125V0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(291.66722, 226.635395)"><g><path d="M2.640625.109375c-.46875.0-.875-.078125-1.21875-.234375S.8125-.5.625-.78125C.445312-1.070312.359375-1.410156.359375-1.796875h1.25c0 .273437.09375.484375.28125.640625.1875.148438.429687.21875.734375.21875.269531.0.484375-.050781.640625-.15625.15625-.101562.234375-.25.234375-.4375.0-.195312-.09375-.359375-.28125-.484375s-.480469-.226563-.875-.3125C1.6875-2.472656 1.195312-2.6875.875-2.96875.5625-3.25.40625-3.617188.40625-4.078125c0-.519531.1875-.925781.5625-1.21875C1.34375-5.597656 1.851562-5.75 2.5-5.75c.445312.0.832031.078125 1.15625.234375.332031.15625.585938.375.765625.65625.175781.273437.265625.59375.265625.96875H3.453125c0-.25-.0859369999999999-.445313-.25-.59375-.15625-.144531-.386719-.21875-.6875-.21875-.273437.0-.484375.0546870000000004-.640625.15625-.148438.105469-.21875.25-.21875.4375.0.179687.082031.324219.25.4375.175781.117187.476562.226563.90625.328125.695312.167969 1.203125.390625 1.515625.671875s.46875.648437.46875 1.09375c0 .53125-.195313.949219-.578125 1.25-.386719.2929688-.914062.4375-1.578125.4375zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(298.776183, 226.635395)"><g><path d="M2.03125.0V-4.4375H.234375V-5.625H5.09375v1.1875H3.3125V0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(306.06207, 226.635395)"><g><path d="M.671875.0V-5.625h2.40625c.425781.0.796875.078125 1.109375.234375.320312.148437.566406.367187.734375.65625.175781.292969.265625.640625.265625 1.046875s-.09375.761719-.28125 1.0625c-.1875.292969-.449219.515625-.78125.671875L5.28125.0H3.8125L2.796875-1.75h-.84375V0zM1.9375-2.828125h.96875c.320312.0.566406-.0703130000000001.734375-.21875.164063-.15625.25-.367187.25-.640625.0-.269531-.0859369999999999-.476562-.25-.625-.167969-.144531-.414063-.21875-.734375-.21875H1.9375zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(313.677673, 226.635395)"><g><path d="M2.15625.0V-1.96875L.078125-5.625H1.53125l.703125 1.296875.5625 1.015625.546875-1.015625L4.046875-5.625H5.5L3.421875-1.984375V0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(113.010659, 234.991643)"><g><path d="M.21875.0l1-4.9375H4.453125l-.1875.953125H2.15625L1.953125-2.96875H3.875l-.1875.9375H1.765625L1.546875-.953125h2.1875L3.546875.0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(117.285054, 234.991643)"><g><path d="M2 .09375c-.429688.0-.78125-.0625-1.0625-.1875C.65625-.226562.453125-.421875.328125-.671875c-.117187-.25-.136719-.546875-.0625-.890625H1.34375C1.3125-1.332031 1.351562-1.148438 1.46875-1.015625c.125.125.328125.1875.609375.1875.238281.0.4375-.046875.59375-.140625s.25-.21875.28125-.375c.0390630000000001-.175781.0-.320312-.125-.4375-.125-.113281-.355469-.207031-.6875-.28125C1.597656-2.175781 1.195312-2.363281.9375-2.625.6875-2.894531.609375-3.253906.703125-3.703125c.070313-.40625.285156-.726563.640625-.96875.351562-.238281.8125-.359375 1.375-.359375.601562.0 1.054688.148438 1.359375.4375.300781.292969.414063.683594.34375 1.171875H3.328125C3.367188-3.628906 3.332031-3.796875 3.21875-3.921875 3.101562-4.054688 2.910156-4.125 2.640625-4.125c-.230469.0-.421875.046875-.578125.140625-.148438.09375-.234375.214844-.265625.359375-.03125.1875.015625.339844.140625.453125.125.105469.359375.195313.703125.265625.550781.125.945313.328125 1.1875.609375.25.273437.332031.617187.25 1.03125-.0859369999999999.417969-.308594.75-.671875 1C3.050781-.0234375 2.582031.09375 2 .09375zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(121.791826, 234.991643)"><g><path d="M1.390625.0l.78125-3.890625H.625L.828125-4.9375h4.25l-.21875 1.046875H3.3125L2.53125.0zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(126.46056, 234.991643)"><g><path d="M.21875.0l1-4.9375H3.125c.507812.0.941406.117188 1.296875.34375.351563.230469.601563.539062.75.921875.144531.386719.171875.835937.078125 1.34375-.09375.46875-.28125.882813-.5625 1.234375-.28125.34375-.636719.617188-1.0625.8125C3.195312-.09375 2.722656.0 2.203125.0zM1.5625-1.03125h.765625c.289063.0.5625-.0546880000000001.8125-.171875.25-.125.457031-.296875.625-.515625.175781-.21875.289063-.472656.34375-.765625.09375-.4375.03125-.78125-.1875-1.03125-.210937-.257813-.53125-.390625-.96875-.390625h-.8125zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(113.166531, 241.44661)"><g><path d="M0 0 .140625-.796875l2.375-1.734375c.21875-.164062.378906-.316406.484375-.453125.113281-.132813.179688-.273437.203125-.421875C3.242188-3.613281 3.210938-3.773438 3.109375-3.890625c-.105469-.125-.261719-.1875-.46875-.1875-.230469.0-.429687.0742190000000003-.59375.21875-.15625.136719-.265625.328125-.328125.578125H.6875c.113281-.550781.347656-.976562.703125-1.28125.351563-.3125.800781-.46875 1.34375-.46875.363281.0.671875.0742190000000003.921875.21875.257812.136719.445312.328125.5625.578125.113281.25.132812.539063.0625.859375-.0625.292969-.1875.570312-.375.828125-.1875.25-.46875.507813-.84375.765625L2.015625-.984375h1.875L3.703125.0zM0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(117.708511, 241.44661)"><g><path d="M2.171875.09375c-.4375.0-.804687-.1015625-1.09375-.3125C.785156-.4375.585938-.738281.484375-1.125c-.105469-.394531-.105469-.867188.0-1.421875C.640625-3.328125.9375-3.9375 1.375-4.375c.445312-.4375.992188-.65625 1.640625-.65625.445313.0.816406.109375 1.109375.328125.289062.210937.488281.511719.59375.90625C4.820312-3.398438 4.816406-2.925781 4.703125-2.375 4.546875-1.601562 4.25-1 3.8125-.5625S2.828125.09375 2.171875.09375zM1.578125-2.4375c-.105469.492188-.101563.871094.015625 1.140625.125.261719.347656.390625.671875.390625.332031.0.613281-.132812.84375-.40625.238281-.28125.410156-.671875.515625-1.171875.09375-.5.078125-.878906-.046875-1.140625C3.460938-3.894531 3.25-4.03125 2.9375-4.03125c-.335938.0-.621094.140625-.859375.421875-.242187.28125-.40625.671875-.5 1.171875zm0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(122.651875, 241.44661)"><g><path d="M0 0 .140625-.796875l2.375-1.734375c.21875-.164062.378906-.316406.484375-.453125.113281-.132813.179688-.273437.203125-.421875C3.242188-3.613281 3.210938-3.773438 3.109375-3.890625c-.105469-.125-.261719-.1875-.46875-.1875-.230469.0-.429687.0742190000000003-.59375.21875-.15625.136719-.265625.328125-.328125.578125H.6875c.113281-.550781.347656-.976562.703125-1.28125.351563-.3125.800781-.46875 1.34375-.46875.363281.0.671875.0742190000000003.921875.21875.257812.136719.445312.328125.5625.578125.113281.25.132812.539063.0625.859375-.0625.292969-.1875.570312-.375.828125-.1875.25-.46875.507813-.84375.765625L2.015625-.984375h1.875L3.703125.0zM0 0"/></g></g></g><g fill="#2e2e2e" fill-opacity="1"><g transform="translate(127.193855, 241.44661)"><g><path d="M0 0 .140625-.796875l2.375-1.734375c.21875-.164062.378906-.316406.484375-.453125.113281-.132813.179688-.273437.203125-.421875C3.242188-3.613281 3.210938-3.773438 3.109375-3.890625c-.105469-.125-.261719-.1875-.46875-.1875-.230469.0-.429687.0742190000000003-.59375.21875-.15625.136719-.265625.328125-.328125.578125H.6875c.113281-.550781.347656-.976562.703125-1.28125.351563-.3125.800781-.46875 1.34375-.46875.363281.0.671875.0742190000000003.921875.21875.257812.136719.445312.328125.5625.578125.113281.25.132812.539063.0625.859375-.0625.292969-.1875.570312-.375.828125-.1875.25-.46875.507813-.84375.765625L2.015625-.984375h1.875L3.703125.0zM0 0"/></g></g></g></svg></a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Open Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Close Menu</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left"><li class=nav-item><a class=nav-link href=/>Home</a></li><li class=nav-item><a class=nav-link href=/docs/>Documentation</a></li><li class=nav-item><a class=nav-link href=/blog/>Publications</a></li><li class=nav-item><a class=nav-link href=/showcase/>Showcase</a></li><li class=nav-item><a class=nav-link href=/community/>Community</a></li><li class="mt-4 inline-block lg:hidden"><a href=https://hugoblox.com/templates/details/docs/>Get Started</a></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><button aria-label=search class="text-black hover:text-primary inline-block px-3 text-xl dark:text-white" id=search_toggle><svg height="16" width="16" viewBox="0 0 512 512" fill="currentcolor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8.0 45.3s-32.8 12.5-45.3.0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9.0 208S93.1.0 208 0 416 93.1 416 208zM208 352a144 144 0 100-288 144 144 0 100 288z"/></svg></button><div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><button class="theme-toggle mt-1" accesskey=t title=appearance><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:hidden"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:block [&:not(dark)]:hidden"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div><a href=https://hugoblox.com/templates/details/docs/ class="inline-block rounded border px-5 py-2 font-semibold transition
md:ml-4 px-4 py-1.5 text-sm
border-black hover:bg-black dark:hover:bg-white dark:hover:text-black hover:text-white dark:border-white dark:text-white dark:hover:bg-white
hidden lg:inline-block">Get Started</a></div></nav></header><div id=search class="hidden p-3"></div></div><div class="page-body my-10"><div class="mx-auto flex max-w-screen-xl"><div class="hb-sidebar-mobile-menu fixed inset-0 z-10 bg-white dark:bg-black/80 hidden"></div><aside class="hb-sidebar-container max-lg:[transform:translate3d(0,-100%,0)] lg:sticky"><div class="px-4 pt-4 lg:hidden"></div><div class="hb-scrollbar lg:h-[calc(100vh-var(--navbar-height))]"><ul class="flex flex-col gap-1 lg:hidden"><li><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/showcase/>Showcase
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/showcase/onlinevqe-copie-2/>Mastering Quantum Chemistry with OpenVQE - A Community-Driven Guide to Quantum Computing</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/showcase/onlinevqe-copie/>Quantum and the Future of Computing Summit</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/showcase/onlinevqe/>Quantum Computing for Quantum Chemistry A Review of UCC Methods and Adapt-VQE Algorithms</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/showcase/vnqa/>Vietnam School of Artificial Intelligence and Quantum Computing</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/showcase/troyes/>SUMMER SCHOOL - QUANTUM, FROM THE LAB TO NEW TECHNOLOGIES</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/showcase/india/>Opensource VQE extension of QLM</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/showcase/qis/>Quantum Innovation Summit</a></li></ul></div></li><li><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/blog/>Published Announcements
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/blog/v3.0.0/>Release second version of OpenVQE package</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/blog/v2.0.0/>The Journal of Physical Chemistry A, 2023, Vol 127/Issue 15, 3543–3550</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/blog/v1.0.0/>WIREs Computational Molecular ScienceVolume 13, Issue 5 e1664</a></li></ul></div></li><li class=open><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/>Documentation
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/getting-started/>Getting Started</a></li><li class="flex flex-col open"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/>Quantum Program
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/project-structure/>Package Structure</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/configuration/>Citing</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/>A - OpenVQE Notebooks
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/quantum_circuit/>1- Circuit-based quantum programming</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/toggle/>2- MyQLM-Fermion</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/observable/>2a-Observables</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/cards/>3- OpenVQE Overall</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/adapt/>4- Adapt-VQE</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/steps/>5- Parameter-Shift Rule</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/ssvqe/>6- SSVQE</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/uscc/>7- USCC</a></li></ul></div></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes_1/>B - Applications
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes_1/battery/>8- Lithium-ion batteries application</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes_1/maxcut/>9- Maxcut by Quantum annealing</a></li></ul></div></li><li class="flex flex-col open"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes_2/>C- Machine Learning Application
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col open"><a class="hb-sidebar-custom-link
sidebar-active-item bg-primary-100 font-semibold text-primary-800 dark:bg-primary-300 dark:text-primary-900" href=/docs/guide/shortcodes_2/machine_learning_tutorial_1/>11- Classical Machine Learning Tutorial 1</a><ul class=hb-sidebar-mobile-toc><li><a href=#introduction class=hb-docs-link>Introduction</a></li><li><a href=#conclusion class=hb-docs-link>Conclusion</a></li></ul></li></ul></div></li></ul></div></li></ul></div></li><li><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/community/>OpenVQA Community</a></li><li class="[word-break:break-word] mt-5 mb-2 px-2 py-1.5 text-sm font-semibold text-gray-900 first:mt-0 dark:text-gray-100 cursor-default"><span>Still need help?</span></li><li><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/community/>Community</a></li></ul><ul class="flex flex-col gap-1 max-lg:hidden"><li><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/getting-started/>Getting Started</a></li><li class=open><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/>Quantum Program
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/project-structure/>Package Structure</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/configuration/>Citing</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/>A - OpenVQE Notebooks
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/quantum_circuit/>1- Circuit-based quantum programming</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/toggle/>2- MyQLM-Fermion</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/observable/>2a-Observables</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/cards/>3- OpenVQE Overall</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/adapt/>4- Adapt-VQE</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/steps/>5- Parameter-Shift Rule</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/ssvqe/>6- SSVQE</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes/uscc/>7- USCC</a></li></ul></div></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes_1/>B - Applications
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes_1/battery/>8- Lithium-ion batteries application</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes_1/maxcut/>9- Maxcut by Quantum annealing</a></li></ul></div></li><li class="flex flex-col open"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/docs/guide/shortcodes_2/>C- Machine Learning Application
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col open"><a class="hb-sidebar-custom-link
sidebar-active-item bg-primary-100 font-semibold text-primary-800 dark:bg-primary-300 dark:text-primary-900" href=/docs/guide/shortcodes_2/machine_learning_tutorial_1/>11- Classical Machine Learning Tutorial 1</a></li></ul></div></li></ul></div></li><li class="[word-break:break-word] mt-5 mb-2 px-2 py-1.5 text-sm font-semibold text-gray-900 first:mt-0 dark:text-gray-100 cursor-default"><span>Still need help?</span></li><li><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/community/>Community</a></li></ul></div></aside><nav class="hb-toc order-last hidden w-64 shrink-0 xl:block print:hidden px-4" aria-label="table of contents"><div class="hb-scrollbar text-sm [hyphens:auto] sticky top-16 overflow-y-auto pr-4 pt-6 max-h-[calc(100vh-var(--navbar-height)-env(safe-area-inset-bottom))] -mr-4 rtl:-ml-4"><p class="mb-4 font-semibold tracking-tight">On this page</p><ul><li class="my-2 scroll-my-6 scroll-py-6"><a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#introduction>Introduction</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#a-construction-of-a-machine-learning-algoritm-by-hand>A construction of a machine learning algoritm by hand</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#support-vector-classifier>Support vector classifier</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#logistic-regression>Logistic Regression</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#conclusion-and-take-home-message>Conclusion and take home message</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#data-vizualization>Data Vizualization</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#feature-engineering>Feature engineering</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#support-vector-classifier-in-transformed-space>Support vector classifier in transformed space</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#achieving-a-nonlinear-boundary-using-a-linear-approach>Achieving a nonlinear boundary using a linear approach</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#kernel-methods-extending-linear-classification-to-nonlinear-problems>Kernel Methods: Extending Linear Classification to Nonlinear Problems</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#conclusion-on-nonlinear-classification-and-the-non-edibility-of-plants>Conclusion on nonlinear classification and the non edibility of plants</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#conclusion>Conclusion</a></li></ul><div class="mt-8 border-t dark:border-neutral-700 pt-8 sticky bottom-0 flex flex-col items-start gap-2 pb-8"><a class="text-xs font-medium text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-100 contrast-more:text-gray-800 contrast-more:dark:text-gray-50" href=https://github.com/HugoBlox/hugo-blox-builder/edit/main/starters/documentation/content/docs/guide/shortcodes_2/Machine_Learning_Tutorial_1.md target=_blank rel=noreferer>Edit this page</a></div></div></nav><article class="flex w-full min-w-0 min-h-[calc(100vh-var(--navbar-height))] justify-center break-words pb-8 pr-[calc(env(safe-area-inset-right)-1.5rem)]"><main class="prose prose-slate lg:prose-xl dark:prose-invert w-full min-w-0 max-w-6xl px-6 pt-4 md:px-12"><div class=mb-1><div class="mt-1.5 flex items-center gap-1 overflow-hidden text-sm text-gray-500 dark:text-gray-400"><div class="whitespace-nowrap transition-colors min-w-[24px] overflow-hidden text-ellipsis hover:text-gray-900 dark:hover:text-gray-100"><a href=http://localhost:56235/docs/>Documentation</a></div><svg class="w-3.5 shrink-0" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m8.25 4.5 7.5 7.5-7.5 7.5"/></svg><div class="whitespace-nowrap transition-colors min-w-[24px] overflow-hidden text-ellipsis hover:text-gray-900 dark:hover:text-gray-100"><a href=http://localhost:56235/docs/guide/>Quantum Program</a></div><svg class="w-3.5 shrink-0" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m8.25 4.5 7.5 7.5-7.5 7.5"/></svg><div class="whitespace-nowrap transition-colors min-w-[24px] overflow-hidden text-ellipsis hover:text-gray-900 dark:hover:text-gray-100"><a href=http://localhost:56235/docs/guide/shortcodes_2/>C- Machine Learning Application</a></div><svg class="w-3.5 shrink-0" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m8.25 4.5 7.5 7.5-7.5 7.5"/></svg><div class="whitespace-nowrap font-medium text-gray-700 dark:text-gray-100">11- Classical Machine Learning Tutorial 1</div></div></div><div class="content text-base"><h1>Classical Machine Learning Tutorial 1</h1><h2 id=introduction>Introduction</h2><p>Welcome to this introductory course on <strong>Machine Learning (ML)</strong>, where we will explore the fundamental concepts and techniques that are the building blocks for more advanced fields, such as <strong>Quantum Machine Learning</strong>. This course is designed with a hands-on approach, offering you the opportunity to engage with real code and solve two simple classification problems.</p><p>The purpose of this course is to provide a foundation in classical machine learning methods, using tools and libraries that are widely applied in the field. One such tool we&rsquo;ll be using is the <strong>scikit-learn</strong> library, particularly the <code>PolynomialFeatures</code> class from <code>sklearn.preprocessing</code>, which allows us to easily transform input data by adding polynomial features to our models. This technique is pivotal for handling non-linear relationships in data and will help us tackle problems more effectively.</p><p>By the end of this course, you will have gained practical experience with popular ML algorithms and tools such as <strong>logistic regression</strong>, <strong>SVMs</strong>, <strong>polynomial classification</strong>, and <strong>kernel methods</strong>, and a deeper understanding of the underlying principles that power these techniques. This knowledge will serve as a stepping stone for your journey into <strong>Quantum Machine Learning</strong>, where quantum computers are used to process data in fundamentally different ways.</p><p>Throughout this course, we will guide you through intuitive, hands-on problems, where you will first see how machine learning models are built from scratch, providing a strong foundational understanding. You will then apply the powerful tools of the <strong>scikit-learn</strong> library to experiment with different models, note their performance, and observe how machine learning algorithms learn and adapt to data. We will emphasize the core philosophy of <strong>learning from data</strong>, which is central to the field of machine learning. By the end of the course, you will be equipped with the practical knowledge and skills to tackle more complex real-world challenges.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LinearRegression</span><span class=p>,</span> <span class=n>LogisticRegression</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>PolynomialFeatures</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>svm</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.svm</span> <span class=kn>import</span> <span class=n>SVC</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>accuracy_score</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>mpl_toolkits.mplot3d</span> <span class=kn>import</span> <span class=n>Axes3D</span>
</span></span></code></pre></div><p>##Toxicity of plants and linear classification</p><p>Imagine biologists working in a remote research lab, deep in the heart of a dense forest. Fascinated by the plant life around, wonder if certain physical traits of plants—specifically the size of their leaves—might reveal clues about their toxicity. To investigate, they measure the width and radius of leaves from various plant species, meticulously recording the data alongside known information about each plant’s toxicity. Their hypothesis is that larger or smaller leaves may correlate with whether a plant is safe to eat or potentially harmful. Our task is to help the biologist analyze this data, using statistical techniques to classify the plants based on their measurements. By finding a predictive link, we aim to offer the biologist a useful model for distinguishing edible plants from toxic ones, guiding safer exploration of the forest&rsquo;s plant diversity.</p><p>###Data vizualization</p><p>The sizes of the leaves are given in the following dataset. To upload them run the code below.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Dataset: Width, Radius, Toxicity</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>5.1</span><span class=p>,</span> <span class=mf>2.0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>4.8</span><span class=p>,</span> <span class=mf>1.8</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>5.0</span><span class=p>,</span> <span class=mf>2.1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.5</span><span class=p>,</span> <span class=mf>3.0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.8</span><span class=p>,</span> <span class=mf>3.2</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.7</span><span class=p>,</span> <span class=mf>3.1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>4.5</span><span class=p>,</span> <span class=mf>1.7</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>5.4</span><span class=p>,</span> <span class=mf>2.3</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.2</span><span class=p>,</span> <span class=mf>2.8</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.6</span><span class=p>,</span> <span class=mf>3.0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>5.3</span><span class=p>,</span> <span class=mf>2.2</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>4.7</span><span class=p>,</span> <span class=mf>1.9</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>7.1</span><span class=p>,</span> <span class=mf>3.4</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>5.2</span><span class=p>,</span> <span class=mf>2.1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.9</span><span class=p>,</span> <span class=mf>3.3</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>5.6</span><span class=p>,</span> <span class=mf>2.4</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>4.6</span><span class=p>,</span> <span class=mf>1.6</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>7.0</span><span class=p>,</span> <span class=mf>3.2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span></code></pre></div><p>Before delving into deeper analysis, it&rsquo;s helpful to start by visualizing the data. Here, we plot the distribution of the leaves in the (width, radius) plane, with colors indicating their toxicity.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Separate features and labels</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:,</span> <span class=p>:</span><span class=mi>2</span><span class=p>]</span>  <span class=c1># Width and Radius as features</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:,</span> <span class=mi>2</span><span class=p>]</span>   <span class=c1># Toxicity as labels</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the data points</span>
</span></span><span class=line><span class=cl><span class=n>scatter</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;coolwarm&#34;</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add colorbar to indicate toxic (1) and edible (0)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>scatter</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span><span class=o>.</span><span class=n>set_ticks</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>  <span class=c1># Set ticks to represent edible (0) and toxic (1)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span><span class=o>.</span><span class=n>set_ticklabels</span><span class=p>([</span><span class=s2>&#34;Edible&#34;</span><span class=p>,</span> <span class=s2>&#34;Toxic&#34;</span><span class=p>])</span>  <span class=c1># Label the colorbar ticks</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Width (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Radius (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Data visualization (Toxic vs. Edible)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;1.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/1.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><h3 id=a-construction-of-a-machine-learning-algoritm-by-hand>A construction of a machine learning algoritm by hand</h3><p>We observe that the leaves are located close to the line:</p>$$
\frac{\textrm{Radius}}{\textrm{Width}} = \textrm{constant}.
$$<p>Additionally, the magnitudes of the vector $(\textrm{width}, \textrm{radius})$ are small for non-toxic plants and large for toxic ones.</p><p>One way to classify the plants could be as follows:</p><ul><li>First, draw the line of constant ratio $\textrm{radius}/\textrm{width}$.</li><li>Then, draw the perpendicular to this line such that it separates the toxic and non-toxic leaves.</li></ul><p>Now, let’s dive into finding the line of constant ratio. What we want is a line that best fits the data points. To find this line, we perform <strong>regression</strong>.</p><p><em>How do we perform the regression?</em></p><p>A mathematical formulation of the problem is to find $\beta_0$ and $\beta_1 \neq 0$ that minimize the given loss function.</p><p>The goal of linear regression is to find the coefficients $ \beta_0 $ (intercept) and $ \beta_1$ (slope) that minimize the sum of squared errors (or residuals). The cost function is given by:</p>$$
J(\beta_0, \beta_1) = \frac{1}{2} \sum_{i=1}^{n} \left( y_i - \left( \beta_0 + \beta_1 x_i \right) \right)^2
$$<p>where:</p><ul><li>$ y_i $ is the radius of the $ i $-th leaf,</li><li>$ x_i $ is the width of the $ i $-th leaf,</li><li>$ \beta_0 $ is the intercept,</li><li>$ \beta_1 $ is the slope.</li></ul><p><em>Remark: You might wonder why we chose the sum of squared errors instead of absolute values. The reason is that the squared error gives us a convex and differentiable function, which is easier to minimize. The coefficient $\frac{1}{2}$ has no effect on the values of $ \beta_0 $ and $ \beta_1$, it is simply used to simplify the computation.</em></p><p>We begin by expanding the cost function:</p>$$
J(\beta_0, \beta_1) = \frac{1}{2} \sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 x_i \right)^2
$$<p>To minimize this function with respect to $ \beta_0 $ and $ \beta_1 $, we take the partial derivatives of $ J(\beta_0, \beta_1) $ with respect to both coefficients and set them equal to zero.</p><ol><li>First, take the derivative with respect to $ \beta_0 $:</li></ol>$$
\frac{\partial J(\beta_0, \beta_1)}{\partial \beta_0} = -\sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 x_i \right)
$$<p>Set this equal to zero to minimize the function:</p>$$
\sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 x_i \right) = 0
$$<p>Expanding this:</p>$$
\sum_{i=1}^{n} y_i - n \beta_0 - \beta_1 \sum_{i=1}^{n} x_i = 0
$$<p>Solving for $ \beta_0 $:</p>$$
n \beta_0 = \sum_{i=1}^{n} y_i - \beta_1 \sum_{i=1}^{n} x_i
$$
$$
\beta_0 = \frac{1}{n} \sum_{i=1}^{n} y_i - \beta_1 \frac{1}{n} \sum_{i=1}^{n} x_i
$$<p>This gives a relationship for $ \beta_0 $ in terms of $ \beta_1 $, the mean of $ x $, and the mean of $ y $.</p><ol start=2><li>Now, take the derivative with respect to $ \beta_1 $:</li></ol>$$
\frac{\partial J(\beta_0, \beta_1)}{\partial \beta_1} = - \sum_{i=1}^{n} x_i \left( y_i - \beta_0 - \beta_1 x_i \right)
$$<p>Set this derivative equal to zero:</p>$$
\sum_{i=1}^{n} x_i \left( y_i - \beta_0 - \beta_1 x_i \right) = 0
$$<p>Expanding:</p>$$
\sum_{i=1}^{n} x_i y_i - \beta_0 \sum_{i=1}^{n} x_i - \beta_1 \sum_{i=1}^{n} x_i^2 = 0
$$<p>Solving for $ \beta_1 $:</p>$$
\beta_1 = \frac{\sum_{i=1}^{n} x_i y_i - \frac{1}{n} \sum_{i=1}^{n} x_i \sum_{i=1}^{n} y_i}{\sum_{i=1}^{n} x_i^2 - \frac{1}{n} \left( \sum_{i=1}^{n} x_i \right)^2}
$$<p>This gives the expression for the slope $ \beta_1 $.</p><p><strong>Conclusion</strong> The coefficients $\beta_0$ and $\beta_1$ are given by the following formulas:</p>$$
\beta_1 = \frac{\sum_{i=1}^{n} x_i y_i - \frac{1}{n} \sum_{i=1}^{n} x_i \sum_{i=1}^{n} y_i}{\sum_{i=1}^{n} x_i^2 - \frac{1}{n} \left( \sum_{i=1}^{n} x_i \right)^2}
$$
$$
\beta_0 = \frac{1}{n} \sum_{i=1}^{n} y_i - \beta_1 \frac{1}{n} \sum_{i=1}^{n} x_i
$$<p>These formulas provide the least squares estimates for the coefficients of a simple linear regression model. We plot this regression line in the code below:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Use Width (X[:, 0]) as the independent variable to compute β_0 and β_1</span>
</span></span><span class=line><span class=cl><span class=n>x_vals</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>y_vals</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate beta_1</span>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>x_vals</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>beta_1</span> <span class=o>=</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>x_vals</span> <span class=o>*</span> <span class=n>y_vals</span><span class=p>)</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>x_vals</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>y_vals</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>x_vals</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>x_vals</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>x_vals</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate beta_0</span>
</span></span><span class=line><span class=cl><span class=n>beta_0</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>y_vals</span><span class=p>)</span> <span class=o>-</span> <span class=n>beta_1</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>x_vals</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the data points</span>
</span></span><span class=line><span class=cl><span class=n>scatter</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;coolwarm&#34;</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add colorbar to indicate toxic (1) and edible (0)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>scatter</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span><span class=o>.</span><span class=n>set_ticks</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>  <span class=c1># Set ticks to represent edible (0) and toxic (1)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span><span class=o>.</span><span class=n>set_ticklabels</span><span class=p>([</span><span class=s2>&#34;Edible&#34;</span><span class=p>,</span> <span class=s2>&#34;Toxic&#34;</span><span class=p>])</span>  <span class=c1># Label the colorbar ticks</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the regression line based on y = beta_0 + beta_1 * x</span>
</span></span><span class=line><span class=cl><span class=n>x_line</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>x_vals</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x_vals</span><span class=p>),</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_line</span> <span class=o>=</span> <span class=n>beta_0</span> <span class=o>+</span> <span class=n>beta_1</span> <span class=o>*</span> <span class=n>x_line</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_line</span><span class=p>,</span> <span class=n>y_line</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;black&#34;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s2>&#34;--&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;y = </span><span class=si>{</span><span class=n>beta_0</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> + </span><span class=si>{</span><span class=n>beta_1</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add labels and title</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Width (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Radius (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Data Visualization (Toxic vs. Edible) with Regression Line&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;2.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/2.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>The Python library scikit-learn (sklearn) provides the LinearRegression function, which allows you to find the best-fit line without the need to manually derive the formulas.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Create a linear regression model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Fit the model to the data</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_vals</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>y_vals</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Extract the coefficients</span>
</span></span><span class=line><span class=cl><span class=n>beta_0</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>intercept_</span>  <span class=c1># This is the y-intercept</span>
</span></span><span class=line><span class=cl><span class=n>beta_1</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>    <span class=c1># This is the slope</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the data points</span>
</span></span><span class=line><span class=cl><span class=n>scatter</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;coolwarm&#34;</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add colorbar to indicate toxic (1) and edible (0)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>scatter</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span><span class=o>.</span><span class=n>set_ticks</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>  <span class=c1># Set ticks to represent edible (0) and toxic (1)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span><span class=o>.</span><span class=n>set_ticklabels</span><span class=p>([</span><span class=s2>&#34;Edible&#34;</span><span class=p>,</span> <span class=s2>&#34;Toxic&#34;</span><span class=p>])</span>  <span class=c1># Label the colorbar ticks</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the regression line based on y = beta_0 + beta_1 * x</span>
</span></span><span class=line><span class=cl><span class=n>x_line</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>x_vals</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x_vals</span><span class=p>),</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_line</span> <span class=o>=</span> <span class=n>beta_0</span> <span class=o>+</span> <span class=n>beta_1</span> <span class=o>*</span> <span class=n>x_line</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_line</span><span class=p>,</span> <span class=n>y_line</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;black&#34;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s2>&#34;--&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;y = </span><span class=si>{</span><span class=n>beta_0</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> + </span><span class=si>{</span><span class=n>beta_1</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add labels and title</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Width (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Radius (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Data Visualization (Toxic vs. Edible) with Regression Line&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;3.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/3.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>Now, we draw a perpendicular line that effectively separates the toxic and non-toxic plants.</p><p>This perpendicular line will have the form:</p>$$ y = - \frac{1}{\beta_1} x + \alpha, $$<p>where $ \alpha \in \mathbb{R} $. The value of $ \alpha $ determines the position of the perpendicular. We can choose $ \alpha $ such that this perpendicular intersects the regression line at the midpoint between the widths of the closest toxic and non-toxic leaves.</p><p>To proceed, we first compute the midpoint, defined as:</p>$$ \text{midwidth} = \frac{\text{nontoxic_max_width} + \text{toxic_min_width}}{2}. $$<p>This enables us to solve for \( \alpha \) by setting up the following equation:</p>$$ \alpha = \beta_0 + \beta_1 \cdot \text{midwidth} + \frac{\text{midwidth}}{\beta_1}. $$<p>The code below provides a plot of this perpendicular line.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Compute midpoint (med) for the closest toxic and non-toxic leaves</span>
</span></span><span class=line><span class=cl><span class=n>nontoxic_max_width</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x_vals</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>])</span>  <span class=c1># Max width among edible plants</span>
</span></span><span class=line><span class=cl><span class=n>toxic_min_width</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>x_vals</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>])</span>     <span class=c1># Min width among toxic plants</span>
</span></span><span class=line><span class=cl><span class=n>midpoint</span> <span class=o>=</span> <span class=p>(</span><span class=n>nontoxic_max_width</span> <span class=o>+</span> <span class=n>toxic_min_width</span><span class=p>)</span> <span class=o>/</span> <span class=mi>2</span>  <span class=c1># Midpoint in width</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Determine the y-value of the regression line at the midpoint</span>
</span></span><span class=line><span class=cl><span class=n>y_mid</span> <span class=o>=</span> <span class=n>beta_0</span> <span class=o>+</span> <span class=n>beta_1</span> <span class=o>*</span> <span class=n>midpoint</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate the perpendicular line’s slope and intercept</span>
</span></span><span class=line><span class=cl><span class=n>perpendicular_slope</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span> <span class=o>/</span> <span class=n>beta_1</span>
</span></span><span class=line><span class=cl><span class=n>perpendicular_intercept</span> <span class=o>=</span> <span class=n>y_mid</span> <span class=o>-</span> <span class=n>perpendicular_slope</span> <span class=o>*</span> <span class=n>midpoint</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the data points</span>
</span></span><span class=line><span class=cl><span class=n>scatter</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;coolwarm&#34;</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add colorbar to indicate toxic (1) and edible (0)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>scatter</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span><span class=o>.</span><span class=n>set_ticks</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>  <span class=c1># Set ticks to represent edible (0) and toxic (1)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span><span class=o>.</span><span class=n>set_ticklabels</span><span class=p>([</span><span class=s2>&#34;Edible&#34;</span><span class=p>,</span> <span class=s2>&#34;Toxic&#34;</span><span class=p>])</span>  <span class=c1># Label the colorbar ticks</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the regression line based on y = beta_0 + beta_1 * x</span>
</span></span><span class=line><span class=cl><span class=n>x_line</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>x_vals</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x_vals</span><span class=p>),</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_line</span> <span class=o>=</span> <span class=n>beta_0</span> <span class=o>+</span> <span class=n>beta_1</span> <span class=o>*</span> <span class=n>x_line</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_line</span><span class=p>,</span> <span class=n>y_line</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;black&#34;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s2>&#34;--&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;y = </span><span class=si>{</span><span class=n>beta_0</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> + </span><span class=si>{</span><span class=n>beta_1</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the perpendicular line</span>
</span></span><span class=line><span class=cl><span class=n>y_perpendicular</span> <span class=o>=</span> <span class=n>perpendicular_slope</span> <span class=o>*</span> <span class=n>x_line</span> <span class=o>+</span> <span class=n>perpendicular_intercept</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_line</span><span class=p>,</span> <span class=n>y_perpendicular</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;purple&#34;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s2>&#34;:&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Perpendicular Separator&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add labels and title</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Width (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Radius (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Data Visualization (Toxic vs. Edible) with Regression and Perpendicular Line&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;4.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/4.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>Step by step, we have separated the plants based on their toxicity. The perpendicular separator we defined is called the decision boundary.
For a new plant, it will be classified as toxic if it falls on the toxic side (to the right of the decision boundary) and non-toxic if it falls on the other side.</p><p>In the machine learning algorithm we developed manually, our approach closely resembled that of the <strong>support vector classifier (SVC)</strong>.</p><h3 id=support-vector-classifier>Support vector classifier</h3><p>The SVC is a powerful machine learning algorithm used for binary classification. It aims to find the optimal hyperplane that best separates the data points of two different classes. In the linear case, where the data is linearly separable, the SVC algorithm looks for the hyperplane that maximizes the margin between the two classes.</p><p><strong>Key Concepts:</strong></p><ol><li><p><strong>Hyperplane:</strong></p><ul><li>A hyperplane is a decision boundary that separates data points of different classes.</li><li>In a 2D space, a hyperplane is a line, while in 3D it is a plane, and in higher dimensions, it is a general hyperplane.</li><li>The goal of SVC is to find the hyperplane that best separates the two classes in the feature space.</li></ul></li><li><p><strong>Margin:</strong></p><ul><li>The margin is the distance between the hyperplane and the closest points from each class.</li><li>The SVC algorithm aims to maximize this margin, which is the distance between the hyperplane and the support vectors (the closest data points to the hyperplane).</li><li>A larger margin usually results in better generalization and classification performance.</li></ul></li><li><p><strong>Support Vectors:</strong></p><ul><li>Support vectors are the data points that lie closest to the decision boundary (hyperplane).</li><li>These points are critical for determining the position of the hyperplane, and removing them would change the decision boundary.</li></ul><p>In the case of linearly separable data (where two classes can be separated by a straight line or hyperplane), the SVC algorithm tries to find the hyperplane that <strong>maximizes the margin</strong> between the two classes.</p><ul><li><p>The equation of the hyperplane is represented as:</p>$$
w \cdot x + b = 0
$$<p>where:</p><ul><li>$w$ is the weight vector, which is perpendicular to the hyperplane.</li><li>$x$ is the feature vector.</li><li>$b$ is the bias term that helps shift the hyperplane.</li></ul></li></ul><p>The objective of the SVC is to find the values of $w$ and $b$ such that the margin between the classes is as wide as possible while ensuring that all points are correctly classified.</p><ul><li><p>The margin can be calculated as the distance between the hyperplane and the closest data points. To maximize this margin, we minimize the following objective function:</p>$$
\frac{1}{2} \|w\|^2
$$<p>subject to the constraints that all data points are correctly classified, which can be written as:</p>$$
y_i (w \cdot x_i + b) \geq 1, \quad \forall i
$$<p>Here:</p><ul><li>$y_i$ is the class label of the $i$-th data point (either +1 or -1),</li><li>$x_i$ is the feature vector of the $i$-th data point.</li></ul><p>These constraints ensure that each data point is correctly classified on the correct side of the hyperplane.</p></li></ul></li></ol><p><strong>Solving the Optimization Problem:</strong></p><p>To find the optimal hyperplane, we need to solve this constrained optimization problem. This involves finding the values of $w$ and $b$ that maximize the margin while satisfying the classification constraints.</p><ul><li>The optimization problem can be solved using <strong>Quadratic Programming</strong> or other optimization techniques.</li></ul><p><strong>Final Decision Rule:</strong></p><p>Once the optimal hyperplane is found, the classifier can predict the class of new data points. A new data point $x$ is classified as follows:</p><ul><li>If $w \cdot x + b > 0$, then the point is classified as belonging to class +1.</li><li>If $w \cdot x + b < 0$, then the point is classified as belonging to class -1.</li></ul><p>The decision rule is based on which side of the hyperplane the new point lies on.</p><p>The following code performs an SVC on the plant dataset.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Separate features and labels</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:,</span> <span class=p>:</span><span class=mi>2</span><span class=p>]</span>  <span class=c1># Width and Radius</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:,</span> <span class=mi>2</span><span class=p>]</span>   <span class=c1># Toxicity label</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create and train the SVM model</span>
</span></span><span class=line><span class=cl><span class=n>svm_model</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;linear&#39;</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>  <span class=c1># Linear kernel for linear classification</span>
</span></span><span class=line><span class=cl><span class=n>svm_model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate accuracy on training data</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=n>svm_model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>accuracy</span> <span class=o>=</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>predictions</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Training Accuracy:&#34;</span><span class=p>,</span> <span class=n>accuracy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the decision boundary and the margin</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the data points</span>
</span></span><span class=line><span class=cl><span class=n>scatter</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;coolwarm&#39;</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>scatter</span><span class=p>,</span> <span class=n>ticks</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Toxicity&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Width (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Radius (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Plant Toxicity Classification with SVM&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get the separating hyperplane</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>svm_model</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>  <span class=c1># Coefficients of the decision boundary</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>svm_model</span><span class=o>.</span><span class=n>intercept_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>  <span class=c1># Intercept</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate the slope and intercept for the line in (Width, Radius) space</span>
</span></span><span class=line><span class=cl><span class=n>slope</span> <span class=o>=</span> <span class=o>-</span><span class=n>w</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>/</span> <span class=n>w</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>intercept</span> <span class=o>=</span> <span class=o>-</span><span class=n>b</span> <span class=o>/</span> <span class=n>w</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the decision boundary (hyperplane)</span>
</span></span><span class=line><span class=cl><span class=n>x_vals</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>(),</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>decision_boundary</span> <span class=o>=</span> <span class=n>slope</span> <span class=o>*</span> <span class=n>x_vals</span> <span class=o>+</span> <span class=n>intercept</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_vals</span><span class=p>,</span> <span class=n>decision_boundary</span><span class=p>,</span> <span class=s1>&#39;k-&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Decision Boundary&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate the margin lines (distance from hyperplane)</span>
</span></span><span class=line><span class=cl><span class=n>margin</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>w</span><span class=o>**</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_vals</span><span class=p>,</span> <span class=n>decision_boundary</span> <span class=o>+</span> <span class=n>margin</span><span class=p>,</span> <span class=s1>&#39;k--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Margin +1&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_vals</span><span class=p>,</span> <span class=n>decision_boundary</span> <span class=o>-</span> <span class=n>margin</span><span class=p>,</span> <span class=s1>&#39;k--&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Margin -1&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Show support vectors</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>svm_model</span><span class=o>.</span><span class=n>support_vectors_</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>svm_model</span><span class=o>.</span><span class=n>support_vectors_</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>s</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>facecolors</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>,</span> <span class=n>edgecolors</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>,</span> <span class=n>linewidths</span><span class=o>=</span><span class=mf>1.5</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Support Vectors&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;5.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><pre><code>Training Accuracy: 1.0
</code></pre><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/5.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>The decision boundary we obtained is slightly different from the one generated by our custom algorithm. This algorithm is known as Support Vector Machine (SVM), as the decision boundary is determined by only a few key data points, called support vectors. These support vectors are highlighted with circles in the figure above.</p><h3 id=logistic-regression>Logistic Regression</h3><p>A more machine learning (ML)-oriented approach involves structuring the problem as a well-defined task. This allows us to focus on setting clear objectives, while delegating the computational complexity and problem-solving effort to the algorithm.</p><p>In this example, the task is to classify plants based on their leaf dimensions. Specifically, our goal is to predict whether a new plant is toxic or not, using the measurements of its leaves. In simpler terms, we aim to build a model that can accurately assign plants to their correct category—toxic or non-toxic—with as few errors as possible.</p><p>The central question then becomes: <em>How can we translate this classification task into a mathematical framework?</em> A good starting point is to examine the dataset and formulate a hypothesis about the relationship between the leaf dimensions and the plant&rsquo;s toxicity. By making such an assumption, we can leverage mathematical tools and techniques to derive a model capable of making these predictions.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Separate features and labels</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:,</span> <span class=p>:</span><span class=mi>2</span><span class=p>]</span>  <span class=c1># Width and Radius as features</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:,</span> <span class=mi>2</span><span class=p>]</span>   <span class=c1># Toxicity as labels</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the data points</span>
</span></span><span class=line><span class=cl><span class=n>scatter</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;coolwarm&#34;</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add colorbar to indicate toxic (1) and edible (0)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>scatter</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span><span class=o>.</span><span class=n>set_ticks</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>  <span class=c1># Set ticks to represent edible (0) and toxic (1)</span>
</span></span><span class=line><span class=cl><span class=n>cbar</span><span class=o>.</span><span class=n>set_ticklabels</span><span class=p>([</span><span class=s2>&#34;Edible&#34;</span><span class=p>,</span> <span class=s2>&#34;Toxic&#34;</span><span class=p>])</span>  <span class=c1># Label the colorbar ticks</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Width (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Radius (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Data visualization (Toxic vs. Edible)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;6.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/6.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>Because of the data distribution, a reasonable and effective hypothesis would be the following:</p><p><em>We assume that the toxic and non-toxic plants can be classified into two distinct categories, separated by a straight line.</em></p><p>Why is this hypothesis smart?</p><ol><li><p>It is realistic:<br>By observing the data, we can see that the toxic and non-toxic plants form two clearly distinguishable groups. Importantly, there are no overlapping or intertwined data points from the two classes in the figure above. This visual cue supports the idea that a simple separation might suffice.</p></li><li><p>It proposes linear separability:<br>This hypothesis assumes that a straight line can separate the two categories, an approach commonly referred to in machine learning and data science as the <em>linearly separable hypothesis</em>. This is a smart assumption for two reasons:</p><ul><li><strong>Plausibility</strong>: The hypothesis aligns with the observed data distribution, making it likely to hold true.</li><li><strong>Simplicity</strong>: A linear boundary represents one of the simplest models available. While it is possible to imagine more complex boundaries—such as polynomial or non-linear curves—a simpler model is generally preferable when it suffices. Simpler models are easier to train, interpret, and generalize well to unseen data, making linear separability an ideal starting point.</li></ul></li></ol><p>By starting with this hypothesis, we embrace the principle of parsimony (<em>Occam&rsquo;s Razor</em>), which suggests opting for the simplest explanation or model that adequately explains the data.</p><p><strong>Mathematical Formulation of the Problem</strong></p><p>Based on the hypothesis of linear separability, we aim to find a straight line that separates toxic plants from non-toxic ones. Mathematically, the equation of the decision boundary can be written as:</p>$$ w_1 \cdot x_1 + w_2 \cdot x_2 + b = 0 $$<p>Here:</p><ul><li>$x_1$ and $x_2$ represent the width and radius of a plant&rsquo;s leaf, respectively.</li><li>$w_1$ and $w_2$ are the weights (coefficients) that determine the orientation of the decision boundary.</li><li>$b$ is the bias term, which shifts the decision boundary.</li></ul><p><strong>Classification Rule</strong></p><p>Given this decision boundary, a new plant can be classified as follows:</p><ul><li>If $w_1 \cdot x_1 + w_2 \cdot x_2 + b > 0$, the plant is classified as <strong>toxic</strong> ($y = 1$).</li><li>If $w_1 \cdot x_1 + w_2 \cdot x_2 + b \leq 0$, the plant is classified as <strong>non-toxic</strong> ($y = 0$).</li></ul><p>To achieve perfect classification, we require that the model correctly assigns each plant in the dataset to its respective category. This can be expressed mathematically using the <strong>0/1 loss function</strong>, which measures whether a prediction is correct ($0$ loss) or incorrect ($1$ loss). For a dataset with $n$ samples, the total loss is:</p>$$ L = \sum_{i=1}^n \mathbb{1}_{\{ y_i \neq \hat{y}_i \}} $$<p>Here:</p><ul><li>$y_i$ is the true label (toxic or non-toxic) for the $i$-th plant.</li><li>$\hat{y}_i$ is the predicted label for the $i$-th plant.</li><li>$\mathbb{1}_{\{\cdot\}}$ is the indicator function, which equals $1$ if the condition inside is true, and $0$ otherwise.</li></ul><p>For zero classification error, we require that:</p>$$ \mathbb{1}_{\{ y_i \neq \hat{y}_i \}} = 0 \quad \text{for all } i = 1, 2, \dots, n. $$<p>This means every prediction must match the true label. Translating this into constraints for the decision boundary, we have:</p><ul><li>For toxic plants ($y_i = 1$), we need $w_1 \cdot x_{1i} + w_2 \cdot x_{2i} + b > 0$.</li><li>For non-toxic plants ($y_i = 0$), we need $w_1 \cdot x_{1i} + w_2 \cdot x_{2i} + b \leq 0$.</li></ul><p>The <strong>key challenge</strong> is to find the optimal values of $w_1$, $w_2$, and $b$ that satisfy these constraints for all data points. This ensures that the decision boundary perfectly separates the two classes with zero classification error.</p><p>To solve this challenge, we need to define the loss function more precisely. The loss is a function of $w = (w_1, w_2)$, $b$, and the dataset. However, since the dataset is fixed, we omit its explicit dependence. Additionally, the event $ \mathbb{1}_{\{ y_i \neq \hat{y}_i \}} = 0$ can be rewritten using the boundary expression.</p><p>To achieve this, we note that the classification condition $y_i \neq \hat{y}_i$ depends on the sign of $w \cdot x_i + b$ (the linear score) relative to $y_i$. By rewriting $y_i$ as $2y_i - 1$ (mapping $y_i = 0$ to $-1$ and $y_i = 1$ to $1$), the condition becomes:</p>$$ (2y_i - 1)(w \cdot x_i + b) > 0. $$<p>This expression ensures that the prediction aligns with the true label:</p><ul><li>If $y_i = 1$ (toxic), $2y_i - 1 = 1$, so the condition is $w \cdot x_i + b > 0$, matching the boundary condition.</li><li>If $y_i = 0$ (non-toxic), $2y_i - 1 = -1$, so the condition is $w \cdot x_i + b \leq 0$, again matching the boundary condition.</li></ul><p>To normalize the loss across datasets of different sizes, we divide by $n$, the number of plants. Thus, the loss function becomes:</p>$$ L(w, b) = \frac{1}{n} \sum_{i=1}^n \mathbb{1}_{\{(2y_i - 1)(w \cdot x_i + b) > 0\}} $$<p><em>Observations:</em></p><ol><li><p><em>Equivalence of Formulations</em><br>The condition $(2y_i - 1)(w \cdot x_i + b) > 0$ is equivalent to the true classification $y_i = \hat{y}_i$. When this product is positive, it indicates that the prediction matches the true label, resulting in zero loss for that particular sample. Conversely, if the product is negative, the prediction does not match the true label, which contributes to the loss.</p></li><li><p><em>Normalization by Sample Size</em><br>Dividing by $n$ (the total number of samples) helps standardize the loss value across datasets of different sizes. Without normalization, datasets with more samples would inherently have larger loss values, even if the model’s classification performance remained consistent.</p></li><li><p><em>Discrete Nature of the Loss</em><br>Due to the indicator function $\mathbb{1}$, this loss is non-differentiable, making it challenging to optimize directly. Minimizing the 0/1 loss exactly is typically impractical, and so alternative approaches—such as using differentiable approximations—are common in machine learning. This limitation is what motivates the use of functions like the <strong>hinge loss</strong> or <strong>logistic loss</strong>. These alternatives approximate the 0/1 behavior while enabling efficient optimization.</p></li></ol><p>To address the non-differentiability of the indicator-based loss, we replace the hard decision boundary with a <strong>smooth, probabilistic interpretation</strong> of classification using the sigmoid function:</p>$$ \sigma(x) = \frac{1}{1 + e^{-x}}. $$<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Define the sigmoid function</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>sigmoid</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the 0/1 function</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>zero_one</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>x</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Generate x values</span>
</span></span><span class=line><span class=cl><span class=n>x_values</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>400</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Compute y values for both functions</span>
</span></span><span class=line><span class=cl><span class=n>sigmoid_values</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>x_values</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>zero_one_values</span> <span class=o>=</span> <span class=n>zero_one</span><span class=p>(</span><span class=n>x_values</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the functions</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_values</span><span class=p>,</span> <span class=n>sigmoid_values</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Sigmoid Function&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_values</span><span class=p>,</span> <span class=n>zero_one_values</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;0/1 Function&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>linestyle</span><span class=o>=</span><span class=s1>&#39;--&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add an upward-pointing arrow for &#34;Probability of Right Classification&#34;</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>annotate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Probability of Right Classification&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>xy</span><span class=o>=</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=n>sigmoid</span><span class=p>(</span><span class=mi>4</span><span class=p>)),</span>  <span class=c1># Arrow points at this coordinate</span>
</span></span><span class=line><span class=cl>    <span class=n>xytext</span><span class=o>=</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>),</span>     <span class=c1># Position of the text</span>
</span></span><span class=line><span class=cl>    <span class=n>arrowprops</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>facecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>,</span> <span class=n>shrink</span><span class=o>=</span><span class=mf>0.05</span><span class=p>,</span> <span class=n>width</span><span class=o>=</span><span class=mf>1.5</span><span class=p>,</span> <span class=n>headwidth</span><span class=o>=</span><span class=mi>8</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;center&#39;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add a downward-pointing arrow for &#34;Probability of Misclassification&#34;</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>annotate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Probability of Misclassification&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>xy</span><span class=o>=</span><span class=p>(</span><span class=o>-</span><span class=mi>4</span><span class=p>,</span> <span class=n>sigmoid</span><span class=p>(</span><span class=o>-</span><span class=mi>4</span><span class=p>)),</span>  <span class=c1># Arrow points at this coordinate</span>
</span></span><span class=line><span class=cl>    <span class=n>xytext</span><span class=o>=</span><span class=p>(</span><span class=o>-</span><span class=mi>4</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>),</span>      <span class=c1># Position of the text</span>
</span></span><span class=line><span class=cl>    <span class=n>arrowprops</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>facecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>,</span> <span class=n>shrink</span><span class=o>=</span><span class=mf>0.05</span><span class=p>,</span> <span class=n>width</span><span class=o>=</span><span class=mf>1.5</span><span class=p>,</span> <span class=n>headwidth</span><span class=o>=</span><span class=mi>8</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;center&#39;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add labels and legend</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Output&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Sigmoid and 0/1 Functions&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;7.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/7.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>The sigmoid function maps any real-valued input to the interval $[0, 1]$, which is useful for interpreting the output as a probability. By applying the sigmoid function to the linear expression $(2y_i - 1)(w \cdot x_i + b)$, we can smoothly approximate the classification probability of each sample.
In the context of our plant classification task, where each plant is represented by its leaf size $(x_i, y_i)$, the expression</p>$$ \sigma((2y_i - 1)(w \cdot x_i + b)) $$<p>gives the probability that the plant is well classified. Here, $(2y_i - 1)(w \cdot x_i + b)$ is a measure of how far the sample is from the decision boundary:</p><ul><li>If this value is close to zero, it suggests that the plant is near the decision boundary, and thus the model has low confidence in classifying it as toxic or non-toxic.</li><li>If the value is positive and large, it means that the sample is far from the boundary and the model is very confident that the plant is well classified.</li><li>Conversely, if it is large and negative, it suggests with high confidence that the plant is misclassified.</li></ul><p>Given this probabilistic interpretation, we can define an overall quantity that measures how well our model classifies all samples:</p>$$ \mathbb{P}(w,b) = \prod_{i = 1}^n \sigma((2y_i - 1)(w \cdot x_i + b)) $$<p>This product represents the probability that all plants are classified correctly according to our model. Known as the <strong>likelihood</strong>, this quantity serves as a natural criterion for determining the best decision boundary $(w, b)$. To maximize this probability, we seek the decision boundary that yields the highest likelihood, a process known in statistics as <strong>maximum likelihood estimation (MLE)</strong>. Here, we estimate the parameters of our model by maximizing the likelihood function.</p><p><em>Remark: The idea behind maximizing the likelihood is intuitive. In data science, our objective—illustrated here by the plant classification task—is to extract insights from data. Specifically, we aim to explain what determines toxicity in plants based on observable features. One way to tackle this is by hypothesizing a model, then adjusting it to make it as realistic as possible. But &ldquo;realistic&rdquo; needs clarification, and a practical interpretation is to &ldquo;maximize the probability of observing the data we have.&rdquo;</em></p><p>To achieve this, we often turn to optimization techniques by minimizing the negative logarithm of this probability. The expression becomes:</p>$$
\begin{aligned}
\mathcal{L}(w,b) &= - \sum_{i = 1}^n \log(\sigma((2y_i - 1)(w \cdot x_i + b))) \\[5pt]
&= \sum_{i = 1}^n \log \left(1 - e^{-(2y_i - 1)(w \cdot x_i + b)} \right).
\end{aligned}
$$<p>This expression, known as the <strong>logistic loss</strong>, is differentiable and allows us to use gradient-based optimization methods to find the optimal values of $w$ and $b$.</p><p>By minimizing the logistic loss, we adjust the model parameters so that toxic plants lie well into the positive region and non-toxic plants lie well into the negative region, thus maximizing the probability of correct classification and achieving effective separation between the classes.</p><p>Since the logistic loss is differentiable, we can proceed by performing a <strong>gradient descent</strong> to find the minimum. The gradient is given by:</p>$$
∇ \mathcal{L}(w,b) = \begin{pmatrix}
\frac{\partial \mathcal{L}(w, b)}{\partial w} \\
\frac{\partial \mathcal{L}(w, b)}{\partial b}
\end{pmatrix}.
$$<p>These derivatives provide the direction of steepest descent for the logistic loss, guiding the optimization process towards the values of $w$ and $b$ that minimize the loss and yield the best separation between toxic and non-toxic plants.</p><p>In the code below, we implement logistic regression using the LogisticRegression function from the scikit-learn library. As being said, logistic regression is a powerful classification algorithm commonly used for binary classification tasks, where the goal is to predict one of two possible outcomes. By fitting a linear decision boundary to the training data, the model predicts the probability of an instance belonging to one of the classes. Scikit-learn’s LogisticRegression class automatically applies optimization techniques to find the best-fitting model parameters (weights and bias), which maximize the likelihood of correct predictions. The model output is interpreted as a probability, thanks to the sigmoid function, and we can use it to classify new data points.</p><p>This implementation provides a straightforward and efficient way to train a logistic regression model, evaluate its accuracy, and visualize the decision boundary for classification tasks.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Separate features and labels</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:,</span> <span class=p>:</span><span class=mi>2</span><span class=p>]</span>  <span class=c1># Width and Radius as features</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:,</span> <span class=mi>2</span><span class=p>]</span>   <span class=c1># Toxicity as labels</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize and train the linear classifier (Logistic Regression)</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Predict the labels using the trained model</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>accuracy</span> <span class=o>=</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>predictions</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Accuracy:&#34;</span><span class=p>,</span> <span class=n>accuracy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plotting the decision boundary</span>
</span></span><span class=line><span class=cl><span class=c1># Create a mesh to plot the decision boundary</span>
</span></span><span class=line><span class=cl><span class=n>x_min</span><span class=p>,</span> <span class=n>x_max</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>y_min</span><span class=p>,</span> <span class=n>y_max</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>x_min</span><span class=p>,</span> <span class=n>x_max</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                     <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>y_min</span><span class=p>,</span> <span class=n>y_max</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Predict over the mesh grid</span>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>c_</span><span class=p>[</span><span class=n>xx</span><span class=o>.</span><span class=n>ravel</span><span class=p>(),</span> <span class=n>yy</span><span class=o>.</span><span class=n>ravel</span><span class=p>()])</span>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>Z</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>xx</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the decision boundary and data points</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>contourf</span><span class=p>(</span><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span><span class=p>,</span> <span class=n>Z</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>RdYlBu</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>RdYlBu</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>60</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Width (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Radius (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Linear Classification of Plants (Toxic vs. Edible)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>label</span><span class=o>=</span><span class=s1>&#39;Toxicity&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;8.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><pre><code>Accuracy: 1.0
</code></pre><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/8.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>There are four steps in the alogorithm:</p><ol><li>The first step is to create an instance of the logistic regression model. This is done using the <code>LogisticRegression()</code> function from scikit-learn. <code>model = LogisticRegression()</code></li><li>The next step is to train the model using the training data. We use the <code>fit()</code> method, which takes the <strong>feature data</strong> ($X =(\textrm{width},\textrm{radius})$) and the <strong>target labels</strong> ($y \in \{0,1\}$) to adjust the model’s parameters (weights and bias). <code>model.fit(X,y)</code></li><li>After training the model, we evaluate its performance by visualizing the decision boundary. In 2D, this boundary is a line, and in higher dimensions, it is a hyperplane that separates the two classes. By plotting the decision boundary, we can visually inspect how well the model classifies the data and identify areas where it may struggle (i.e., near the boundary). This helps us assess the model&rsquo;s generalization ability and robustness. We can also computate the <strong>accuracy</strong>,
$$ \textrm{accuracy} = \frac{\textrm{well classified data}}{\textrm{well} + \textrm{misclassified data}}.$$</li></ol><p><em>Remark: It is important to be cautious when using any</em> <strong>metric score</strong>, <em>such as accuracy, to evaluate model performance. In cases of imbalanced datasets, accuracy can provide a misleading assessment of the model&rsquo;s effectiveness. For example, if there are significantly more toxic plants than non-toxic ones in the dataset, a model that predicts &ldquo;toxic&rdquo; for most plants may appear to perform well in terms of accuracy, even though it fails to correctly classify non-toxic plants.</em></p><p>Now that our logistic regression model is trained, we can use it to make predictions for new plants. Given the features of a new plant (e.g., leaf size, width, and radius), we input these values into the trained model to obtain the predicted class. The model will output a probability indicating how likely the plant is to belong to the toxic or non-toxic category. Based on this probability, we can assign the plant to the most likely class.</p><p>For example, we can use the following code to predict the class of a new plant:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Test Dataset: New data to evaluate the classifier</span>
</span></span><span class=line><span class=cl><span class=n>test_data</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>5.7</span><span class=p>,</span> <span class=mf>2.5</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.3</span><span class=p>,</span> <span class=mf>2.9</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>4.9</span><span class=p>,</span> <span class=mf>2.0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.4</span><span class=p>,</span> <span class=mf>3.1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>5.5</span><span class=p>,</span> <span class=mf>2.2</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.7</span><span class=p>,</span> <span class=mf>3.3</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>5.8</span><span class=p>,</span> <span class=mf>2.4</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.1</span><span class=p>,</span> <span class=mf>2.7</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Separate test features and labels</span>
</span></span><span class=line><span class=cl><span class=n>X_test</span> <span class=o>=</span> <span class=n>test_data</span><span class=p>[:,</span> <span class=p>:</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>y_test</span> <span class=o>=</span> <span class=n>test_data</span><span class=p>[:,</span> <span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize and train the linear classifier (Logistic Regression)</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Predict on the test set</span>
</span></span><span class=line><span class=cl><span class=n>test_predictions</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate the accuracy on the test set</span>
</span></span><span class=line><span class=cl><span class=n>test_accuracy</span> <span class=o>=</span> <span class=n>accuracy_score</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>test_predictions</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Test Set Accuracy:&#34;</span><span class=p>,</span> <span class=n>test_accuracy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print predictions for the test set</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Test Predictions:&#34;</span><span class=p>,</span> <span class=n>test_predictions</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Actual Test Labels:&#34;</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plotting the decision boundary with both training and test points</span>
</span></span><span class=line><span class=cl><span class=c1># Create a mesh to plot the decision boundary</span>
</span></span><span class=line><span class=cl><span class=n>x_min</span><span class=p>,</span> <span class=n>x_max</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>y_min</span><span class=p>,</span> <span class=n>y_max</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>x_min</span><span class=p>,</span> <span class=n>x_max</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                     <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>y_min</span><span class=p>,</span> <span class=n>y_max</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Predict over the mesh grid</span>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>c_</span><span class=p>[</span><span class=n>xx</span><span class=o>.</span><span class=n>ravel</span><span class=p>(),</span> <span class=n>yy</span><span class=o>.</span><span class=n>ravel</span><span class=p>()])</span>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>Z</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>xx</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the decision boundary and training points</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>contourf</span><span class=p>(</span><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span><span class=p>,</span> <span class=n>Z</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>RdYlBu</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>RdYlBu</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>60</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;o&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Training Points&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_test</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X_test</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y_test</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=n>plt</span><span class=o>.</span><span class=n>cm</span><span class=o>.</span><span class=n>RdYlBu</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>60</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;s&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Test Points&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Width (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Radius (cm)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Linear Classification of Plants (Toxic vs. non toxic)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>label</span><span class=o>=</span><span class=s1>&#39;Toxicity&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;9.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><pre><code>Test Set Accuracy: 0.5
Test Predictions: [1 1 1 1 1 1 1 1]
Actual Test Labels: [0. 1. 0. 1. 0. 1. 0. 1.]
</code></pre><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/9.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>After training our logistic regression model on the training dataset, we evaluate its performance using a <strong>test dataset</strong>. This evaluation is crucial as it helps us understand how well the model generalizes to new, unseen data—something that is essential for ensuring its real-world applicability. Below is an overview of the key results:</p><ul><li><p><strong>Test Set Performance</strong>: The model achieved <strong>perfect accuracy</strong> on the test set, correctly classifying all test samples as toxic or non-toxic. This suggests that the model has effectively learned the relationship between the input features (leaf size) and the target labels (toxicity) during training and can accurately predict toxicity in new, unseen samples.</p></li><li><p><strong>Decision Boundary and Class Separation</strong>: By visualizing the decision boundary, we can see that the model has created a clear division between the toxic and non-toxic plants, with the test data points also lying on the correct side of the boundary. This visual inspection confirms that the model&rsquo;s decision-making is consistent with the underlying structure of the data.</p></li><li><p><strong>Model Generalization</strong>: The fact that the model performs well on both the training data and the test data suggests that it has generalized well. This is a key aspect of a robust model—ensuring that it does not overfit to the training data, but instead learns patterns that are applicable to any new data it encounters.</p></li></ul><p><strong>Broader Takeaways</strong></p><p>The success of logistic regression in this task highlights the power of such ML algorithm called here <strong>supervised learning algorithms</strong> in classification tasks, especially when the data is well-structured and the model is appropriately regularized. Furthermore, the ability to visualize the <strong>decision boundary</strong> provides valuable insights into the model&rsquo;s behavior, offering a clear explanation of how it classifies new examples.</p><p>In real-world applications, this approach is essential because models that cannot generalize well to unseen data can lead to poor performance when deployed in dynamic environments. The high accuracy and well-separated decision boundary in our case suggest that this logistic regression model would be effective in real-world plant toxicity classification tasks, making it a practical tool for predictive analytics in fields like agriculture or environmental science.</p><h3 id=conclusion-and-take-home-message>Conclusion and take home message</h3><p>The classification task that we performed is known as binary linear classification. It is binary because it involves two distinct classes, and linear because the decision boundary can be represented with a simple linear equation.</p><p>This task captures the essence of machine learning, where models learn to identify patterns from data, extracting knowledge that might otherwise remain hidden. Machine learning does not just automate processes—it helps us interpret, understand, and even predict aspects of reality by leveraging the power of computation.</p><p>Yet, reality is often complex, and the relationships in data are not always clear-cut or easily separable by simple boundaries. For instance, if the plants in our dataset were not linearly separable, we would need more sophisticated techniques to uncover the underlying patterns, perhaps through nonlinear transformations or other feature engineering approaches. This is where the potential of machine learning truly unfolds.</p><p>##Edibility of plants and nonlinear classification</p><p>The team of biologists celebrated their groundbreaking achievement: they had successfully classified toxic and non-toxic plants using the shapes of their leaves through a linear classification model. This breakthrough promised safer ecosystems, as animals and humans could now distinguish between harmful and harmless plants with ease. The team&rsquo;s discovery was heralded as a triumph of science and machine learning.</p><p>However, an alarming phenomenon soon emerged. Entire colonies of ants were mysteriously dying after collecting what the model identified as &ldquo;non-toxic&rdquo; plants. Despite the model&rsquo;s high accuracy in the lab, these plants wreaked havoc in the ants’ colonies. Confused and concerned, the biologists embarked on a deeper investigation.</p><p>To understand the mystery, the team began mapping the geographic locations of both the thriving and decimated ant colonies. Run the code below to import the data:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Function to generate the dataset</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>colonies_locations</span><span class=p>(</span><span class=n>num_points</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span> <span class=n>circle_radius</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>center</span><span class=o>=</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Generate a dataset of ant colonies&#39; locations. Colonies inside the toxic radius
</span></span></span><span class=line><span class=cl><span class=s2>    are marked as affected (1), while others outside are unaffected (0).
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># Generate random points (x, y) in the 2D plane</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=o>-</span><span class=n>circle_radius</span> <span class=o>*</span> <span class=mf>1.5</span><span class=p>,</span> <span class=n>circle_radius</span> <span class=o>*</span> <span class=mf>1.5</span><span class=p>,</span> <span class=p>(</span><span class=n>num_points</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Calculate the Euclidean distance of each point from the center</span>
</span></span><span class=line><span class=cl>    <span class=n>distances</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>X</span> <span class=o>-</span> <span class=n>center</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Label points based on whether they are inside (1) or outside (0) the toxic zone</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=p>(</span><span class=n>distances</span> <span class=o>&gt;</span> <span class=n>circle_radius</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>int</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>colonies_locations</span><span class=p>(</span><span class=n>num_points</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span> <span class=n>circle_radius</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=data-vizualization>Data Vizualization</h3><p>Just as before, a good starting point is to closely examine the dataset. Observing its structure, patterns, and distribution provides valuable insights into the problem at hand and sets the foundation for any subsequent analysis or modeling.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Plot the dataset</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Decimated Colonies&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Thriving Colonies&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Longitude&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Latitude&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Geographical Distribution of Ant Colonies&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>axhline</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>axvline</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;10.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/10.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>To solve this problem using machine learning, we can leverage what we&rsquo;ve learned about linear classification. However, in this case, it&rsquo;s clear that no simple straight line can separate the decimated colonies (red) from the thriving colonies (blue). To overcome this, we can transform the dataset in a way that makes it linearly separable while preserving its inherent structure.</p><h3 id=feature-engineering>Feature engineering</h3><p>One approach is to apply a <strong>feature map</strong>, which is a mathematical function that transforms the data into a higher-dimensional space where the separation becomes easier. Specifically, we define a feature map $\Phi$ from the original 2D space $\mathbb{R}^2$ to a transformed space $\Phi(\mathbb{R}^2)$:</p>$$ \Phi : \mathbb{R}^2 \to \Phi(\mathbb{R}^2) $$<p>The goal is to transform the dataset so that it becomes linearly separable in the new feature space. A smart way to do this is by adding a feature that measures the Euclidean distance from the center point (the origin, in this case). This new feature can help in distinguishing between the colonies that are inside the circle (label 0) and those that are outside (label 1).</p><p>We define the feature map as:</p>$$ \Phi(x_1,x_2) = \left( x_1, x_2, 4(x_1^2 + x_2^2) \right) $$<p>This transformation introduces a new third dimension based on the squared distance from the center of the circle, which can help make the data linearly separable in the transformed space.</p><p>The following plot visualizes the location of the ants&rsquo; colonies after applying the feature map, showing how the transformation helps in distinguishing the two classes more clearly.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Feature map function</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>feature_map</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x1</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>x2</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>column_stack</span><span class=p>((</span><span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>,</span> <span class=mi>4</span><span class=o>*</span><span class=p>(</span><span class=n>x1</span><span class=o>**</span><span class=mi>2</span> <span class=o>+</span> <span class=n>x2</span><span class=o>**</span><span class=mi>2</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply the feature map to the dataset</span>
</span></span><span class=line><span class=cl><span class=n>X_featured</span> <span class=o>=</span> <span class=n>feature_map</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>column_stack</span><span class=p>((</span><span class=n>X_featured</span><span class=p>,</span> <span class=n>y</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plotting in 3D space</span>
</span></span><span class=line><span class=cl><span class=n>fig</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>ax</span> <span class=o>=</span> <span class=n>fig</span><span class=o>.</span><span class=n>add_subplot</span><span class=p>(</span><span class=mi>111</span><span class=p>,</span> <span class=n>projection</span><span class=o>=</span><span class=s1>&#39;3d&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Scatter plot the transformed points</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span>
</span></span><span class=line><span class=cl>           <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Decimated colony&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span>
</span></span><span class=line><span class=cl>           <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Thriving colony&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Labels and title</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;x1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;x2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_zlabel</span><span class=p>(</span><span class=s1>&#39;4*(x1^2 + x2^2)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;3D Feature Space: Transformed Points by Feature Map&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Show legend</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Adjust view to make the red points clearly above the blue points</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>view_init</span><span class=p>(</span><span class=n>elev</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>azim</span><span class=o>=</span><span class=mi>30</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;11.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/11.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><h3 id=support-vector-classifier-in-transformed-space>Support vector classifier in transformed space</h3><p>In the transformed feature space, the data appears to be linearly separable. To leverage the techniques we&rsquo;ve learned in this course, we can use a <strong>Support Vector Classifier (SVC)</strong> from the <strong>scikit-learn</strong> library to find the optimal decision boundary.</p><p>While it is possible to derive the decision boundary manually by maximizing the margin — the distance between the two classes — the focus of this course is not on hand-calculating these values. Instead, we aim to take advantage of simple, efficient methods provided by existing libraries, reserving complex mathematical derivations for when they are necessary.</p><p>The following code demonstrates how to use the SVC to find and plot the separating affine hyperplane that divides the two classes.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Train SVM on the transformed feature space</span>
</span></span><span class=line><span class=cl><span class=n>svm_classifier</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;linear&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>svm_classifier</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_featured</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Extract the separating hyperplane parameters (weights and bias)</span>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>svm_classifier</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>bias</span> <span class=o>=</span> <span class=n>svm_classifier</span><span class=o>.</span><span class=n>intercept_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the decision boundary (hyperplane) equation in the feature space</span>
</span></span><span class=line><span class=cl><span class=c1># Equation of the hyperplane: w1*x1 + w2*x2 + w3*x3 + b = 0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a grid of points in 3D space for plotting</span>
</span></span><span class=line><span class=cl><span class=n>x1_range</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>X_featured</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>X_featured</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>(),</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x2_range</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>X_featured</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>X_featured</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>(),</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x1_grid</span><span class=p>,</span> <span class=n>x2_grid</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>x1_range</span><span class=p>,</span> <span class=n>x2_range</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Compute the corresponding x3 values based on the decision boundary</span>
</span></span><span class=line><span class=cl><span class=n>x3_grid</span> <span class=o>=</span> <span class=o>-</span><span class=p>(</span><span class=n>weights</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=n>x1_grid</span> <span class=o>+</span> <span class=n>weights</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=n>x2_grid</span> <span class=o>+</span> <span class=n>bias</span><span class=p>)</span> <span class=o>/</span> <span class=n>weights</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plotting the data and the separating hyperplane</span>
</span></span><span class=line><span class=cl><span class=n>fig</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>ax</span> <span class=o>=</span> <span class=n>fig</span><span class=o>.</span><span class=n>add_subplot</span><span class=p>(</span><span class=mi>111</span><span class=p>,</span> <span class=n>projection</span><span class=o>=</span><span class=s1>&#39;3d&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the data points</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span>
</span></span><span class=line><span class=cl>           <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Decimated colonies&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>X_featured</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span>
</span></span><span class=line><span class=cl>           <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Thriving colonies&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the separating hyperplane (decision boundary)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>plot_surface</span><span class=p>(</span><span class=n>x1_grid</span><span class=p>,</span> <span class=n>x2_grid</span><span class=p>,</span> <span class=n>x3_grid</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;yellow&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>rstride</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>cstride</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Labels and title</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;x1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;x2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_zlabel</span><span class=p>(</span><span class=s1>&#39;4*(x1^2 + x2^2)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;3D Feature Space: SVM Separating Hyperplane&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Show legend</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Adjust view for better visualization</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>view_init</span><span class=p>(</span><span class=n>elev</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>azim</span><span class=o>=</span><span class=mi>30</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;12.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/12.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><h3 id=achieving-a-nonlinear-boundary-using-a-linear-approach>Achieving a nonlinear boundary using a linear approach</h3><p>To transition from the feature space back to the original geographic space of the colonies (latitude and longitude), we need to determine the boundary $B$ that separates the decimated colonies from the thriving ones in $\mathbb{R}^2$.</p><p>Let $H$ denote the affine separating hyperplane in the feature space. The corresponding boundary $B$ in $\mathbb{R}^2$ is defined as:</p>$$
B = \Phi^{-1} \left( H \cap \Phi \left( \mathbb{R}^2 \right) \right).
$$<p>Why does this definition work? The reasoning is as follows:</p><ol><li>A point $(x_1, x_2)$ lies on the boundary $B$ if and only if its feature-transformed image $\Phi(x_1, x_2)$ lies on the hyperplane $H$:
$$
(x_1, x_2) \in B \iff \Phi(x_1, x_2) \in H.
$$</li><li>Additionally, since $\Phi(x_1, x_2)$ is in the range of $\Phi$, we also have:
$$
\Phi(x_1, x_2) \in H \cap \Phi\left(\mathbb{R}^2\right).
$$</li><li>Applying $\Phi^{-1} : \Phi \left( \mathbb{R}^2 \right) \to \mathbb{R}^2$, which is bijective, we arrive at:
$$
(x_1, x_2) \in \Phi^{-1} \left( H \cap \Phi\left(\mathbb{R}^2\right) \right).
$$</li></ol><p>Thus, the boundary $B$ is precisely the inverse image of the intersection of $H$ with $\Phi(\mathbb{R}^2)$.</p><p>We now derive the boundary</p><p>The feature map is defined as:</p>$$
\Phi(x_1, x_2) = \left(x_1, x_2, 4(x_1^2 + x_2^2)\right).
$$<p>The range of $\Phi$ is:</p>$$
\Phi\left(\mathbb{R}^2\right) = \left\lbrace \left(x_1, x_2, 4(x_1^2 + x_2^2)\right) \mid (x_1, x_2) \in \mathbb{R}^2 \right\rbrace.
$$<p>The separating hyperplane $H$ in the feature space is given by the equation derived from the SVM classifier:</p>$$
H: \left\lbrace (y_1, y_2, y_3) \in \mathbb{R}^3 \mid w_1 y_1 + w_2 y_2 + w_3 y_3 + b = 0 \right\rbrace.
$$<p>The intersection $H \cap \Phi\left(\mathbb{R}^2\right)$ is therefore:</p>$$
H \cap \Phi\left(\mathbb{R}^2\right) = \left\lbrace \left(x_1, x_2, 4(x_1^2 + x_2^2)\right) \mid (x_1, x_2) \in \mathbb{R}^2 \text{ and } w_1 x_1 + w_2 x_2 + 4w_3(x_1^2 + x_2^2) + b = 0 \right\rbrace.
$$<p>Applying $\Phi^{-1}$, we obtain the boundary in $\mathbb{R}^2$:</p>$$
B = \left\lbrace (x_1, x_2) \in \mathbb{R}^2 \mid 4w_3 x_1^2 + 4w_3 x_2^2 + w_1 x_1 + w_2 x_2 + b = 0 \right\rbrace.
$$<p>The resulting equation for $B$ is a <strong>conic section</strong> (e.g., a circle, ellipse, parabola, or hyperbola) depending on the parameters $w_1, w_2, w_3,$ and $b$. This reflects the flexibility of using nonlinear feature transformations, as they allow us to solve problems that are not linearly separable in the original space.</p><p>In this case, the SVM in the feature space finds a linear separator, but when projected back to the original space, the separator becomes a curved boundary, effectively distinguishing the decimated colonies from the thriving ones.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Define a grid for the original space</span>
</span></span><span class=line><span class=cl><span class=n>x1_range</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>(),</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x2_range</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>(),</span> <span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x1_grid</span><span class=p>,</span> <span class=n>x2_grid</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>x1_range</span><span class=p>,</span> <span class=n>x2_range</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Compute the corresponding boundary in the original space</span>
</span></span><span class=line><span class=cl><span class=n>boundary_values</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=mi>4</span> <span class=o>*</span> <span class=n>weights</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=o>*</span> <span class=p>(</span><span class=n>x1_grid</span><span class=o>**</span><span class=mi>2</span> <span class=o>+</span> <span class=n>x2_grid</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>+</span> <span class=n>weights</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=n>x1_grid</span>
</span></span><span class=line><span class=cl>    <span class=o>+</span> <span class=n>weights</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=n>x2_grid</span>
</span></span><span class=line><span class=cl>    <span class=o>+</span> <span class=n>bias</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the dataset and boundary in the original space</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the data points</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Decimated colonies&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Thriving colonies&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the boundary</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>contour</span><span class=p>(</span><span class=n>x1_grid</span><span class=p>,</span> <span class=n>x2_grid</span><span class=p>,</span> <span class=n>boundary_values</span><span class=p>,</span> <span class=n>levels</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>colors</span><span class=o>=</span><span class=s1>&#39;yellow&#39;</span><span class=p>,</span> <span class=n>linewidths</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add labels, title, and legend</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;x1 (Latitude)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;x2 (Longitude)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Original Space: Dataset and Boundary $B$&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;13.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/13.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>The classification results in the transformed feature space are highly successful. By mapping the data to a higher-dimensional space, we were able to achieve a clear separation between the decimated and thriving colonies. The SVM classifier identified a linear hyperplane in this new space, which corresponds to a non-linear decision boundary in the original feature space. This process showcases the effectiveness of <strong>feature engineering</strong> in simplifying complex classification tasks.</p><p>The core idea behind this approach was to modify the feature space such that the complex relationships in the original data become more tractable. Initially, the data in $ \mathbb{R}^2 $ was not linearly separable. To address this, we applied a feature map:</p>$$
\Phi(x_1, x_2) = (x_1, x_2, 4(x_1^2 + x_2^2)),
$$<p><br>which introduces an additional feature that captures the squared Euclidean distance from the origin. This transformation sends the data into a higher-dimensional space $ \mathbb{R}^3 $, where the two classes become linearly separable. After identifying a linear separator (hyperplane) in this new space, we can map the decision boundary back to the original space, where it appears as a non-linear separation.</p><p>The motivation behind transforming the feature space stems from the limitations of linear classifiers. While linear classifiers are powerful and easy to interpret, they struggle with data that is not linearly separable. By mapping the data into a higher-dimensional space, we take advantage of the geometry in that space to make the classification task simpler. This strategy is the essence of <strong>kernel methods</strong>, such as <strong>kernelized SVM</strong>, which extend the concept of linear classification to non-linear problems by implicitly working in a higher-dimensional feature space.</p><h3 id=kernel-methods-extending-linear-classification-to-nonlinear-problems>Kernel Methods: Extending Linear Classification to Nonlinear Problems</h3><p>Kernel methods provide a powerful framework for tackling classification tasks where data is not linearly separable in the original feature space. Instead of explicitly transforming the data into a higher-dimensional space, kernel methods use a mathematical trick that allows us to compute the inner product of data points in this higher-dimensional space without ever computing their explicit coordinates. This trick is known as the <strong>kernel trick</strong>, and it allows us to perform complex non-linear classification efficiently, using simple linear classifiers like Support Vector Machines (SVMs).</p><p>The idea behind kernel methods is to use a <strong>kernel function</strong>, which implicitly maps the data from the original feature space into a higher-dimensional space where linear separation becomes possible. A kernel function computes the inner product between the transformed data points in this higher-dimensional space, without actually performing the transformation. The most commonly used kernel functions are:</p><ol><li><p><strong>Linear Kernel</strong>: This is simply the standard dot product between data points in the original space, and is equivalent to using no transformation at all.</p>$$ K(x, y) = x^T y $$</li><li><p><strong>Polynomial Kernel</strong>: This kernel maps the data to a higher-dimensional space based on polynomial features.</p>$$ K(x, y) = (x^T y + c)^d $$</li><li><p><strong>Radial Basis Function (RBF) Kernel</strong>: Also known as the Gaussian kernel, it maps data to an infinite-dimensional space, and is particularly effective when dealing with complex data distributions.</p>$$ K(x, y) = \exp\left(-\frac{\|x - y\|^2}{2\sigma^2}\right) $$</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Train an SVM classifier with a polynomial kernel (degree 2)</span>
</span></span><span class=line><span class=cl><span class=n>svm_classifier</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;poly&#39;</span><span class=p>,</span> <span class=n>degree</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>C</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>svm_classifier</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plotting the decision boundary in the original space</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Decimated colonies&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Thriving colonies&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a mesh grid for plotting decision boundary</span>
</span></span><span class=line><span class=cl><span class=n>x1_range</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>(),</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x2_range</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>(),</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>x1_range</span><span class=p>,</span> <span class=n>x2_range</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Use the trained SVM to predict on the grid points</span>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>svm_classifier</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>c_</span><span class=p>[</span><span class=n>xx</span><span class=o>.</span><span class=n>ravel</span><span class=p>(),</span> <span class=n>yy</span><span class=o>.</span><span class=n>ravel</span><span class=p>()])</span>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>Z</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>xx</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the decision boundary</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>contour</span><span class=p>(</span><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span><span class=p>,</span> <span class=n>Z</span><span class=p>,</span> <span class=n>levels</span><span class=o>=</span><span class=p>[</span><span class=mf>0.5</span><span class=p>],</span> <span class=n>linewidths</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>colors</span><span class=o>=</span><span class=s1>&#39;yellow&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Labels and title for the original space</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;x1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;x2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Decision Boundary with Polynomial Kernel (Degree 2)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;14.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/14.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>The decision boundary obtained using the polynomial kernel method exhibits slight differences compared to the one derived from the explicit feature mapping and linear classification. These differences stem from the nature of kernel methods and their underlying computations.</p><ul><li><strong>Explicit Feature Map:</strong> When using a hand-crafted feature map (e.g., ($ \Phi(x_1, x_2) = (x_1, x_2, 4(x_1^2 + x_2^2)) $), the transformation is predefined and fixed. The classifier then finds a hyperplane in the explicitly mapped feature space, which corresponds to a non-linear boundary in the original space.</li><li><strong>Kernel Trick:</strong> In contrast, the polynomial kernel implicitly computes the feature interactions without explicitly mapping the data into the higher-dimensional space. This implicit mapping allows the kernel to adaptively capture the data structure based on the degree of the polynomial and the distribution of the data.</li></ul><p>The boundary obtained using the polynomial kernel is influenced by the kernel parameters and the data distribution. The kernel computes similarity directly between points, allowing for a more flexible separation. When using the hand-crafted feature map, the decision boundary is directly tied to the chosen transformation. While effective for specific problems (e.g., circular separability), this approach may lack flexibility if the transformation does not perfectly capture the data&rsquo;s underlying geometry.</p><p>In this example, the explicit feature map emphasizes radial separability with a specific quadratic term ($ 4(x_1^2 + x_2^2) $), while the polynomial kernel considers all possible quadratic combinations of the features. This subtle difference in feature representation affects the decision boundary&rsquo;s shape.</p><p>Despite the slight differences in the boundary, both approaches successfully classify the two classes. However, the polynomial kernel’s <strong>flexibility</strong> ensures <strong>robustness</strong> across various data distributions, which might not always be achievable with a manually defined feature map.</p><p>The differences between the two boundaries highlight the balance between flexibility and <strong>specificity</strong> in feature transformation. While explicit feature maps can provide elegant solutions for specific problems, kernel methods excel in generality and <strong>adaptability</strong>, making them a cornerstone of modern machine learning techniques.</p><p>###Polynomial classification</p><p>Instead of relying on kernel methods, one might choose to leverage the hypothesis on the form of the decision boundary in the original feature space. As mentioned above, in our classification problem, the data suggests a <strong>quadratic separation</strong> between the two classes:</p>$$ \mathcal{B}: \alpha_1 x_1^2 + \alpha_2 x_2^2 + \alpha_3 x_1 x_2 + \alpha_4 x_1 + \alpha_5 x_2 + \beta = 0.$$<p>(To be more specified we can already set $\beta = 0$ since the dataset is centered ouround the origin). This approach involves making an <strong>explicit assumption</strong> about the mathematical structure of the boundary, such as a second-degree polynomial equation, and then determining the optimal parameters of this equation.</p><p>The strategy here mirrors that of logistic regression but applied to an expanded feature space. By generating polynomial features (e.g., quadratic terms), we transform the dataset into a higher-dimensional representation where the decision boundary becomes linear. The parameters of the quadratic decision boundary are then estimated by minimizing the <strong>logistic loss function</strong> over the dataset, as in standard logistic regression.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Create polynomial features (degree 2)</span>
</span></span><span class=line><span class=cl><span class=n>poly</span> <span class=o>=</span> <span class=n>PolynomialFeatures</span><span class=p>(</span><span class=n>degree</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>include_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X_poly</span> <span class=o>=</span> <span class=n>poly</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Train a logistic regression classifier on the expanded polynomial features</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_poly</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a mesh grid for plotting decision boundary</span>
</span></span><span class=line><span class=cl><span class=n>x1_range</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>(),</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x2_range</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>(),</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>(),</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>x1_range</span><span class=p>,</span> <span class=n>x2_range</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>grid_points</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>c_</span><span class=p>[</span><span class=n>xx</span><span class=o>.</span><span class=n>ravel</span><span class=p>(),</span> <span class=n>yy</span><span class=o>.</span><span class=n>ravel</span><span class=p>()]</span>
</span></span><span class=line><span class=cl><span class=n>grid_points_poly</span> <span class=o>=</span> <span class=n>poly</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>grid_points</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Predict probabilities for the grid points</span>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>classifier</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>grid_points_poly</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>Z</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>xx</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the decision boundary</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Decimated colonies&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=n>y</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Thriving colonies&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>contour</span><span class=p>(</span><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span><span class=p>,</span> <span class=n>Z</span><span class=p>,</span> <span class=n>levels</span><span class=o>=</span><span class=p>[</span><span class=mf>0.5</span><span class=p>],</span> <span class=n>linewidths</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>colors</span><span class=o>=</span><span class=s1>&#39;yellow&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;x1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;x2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Decision Boundary with Polynomial Classification (Degree 2)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;15.png&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><figure><div class="flex justify-center"><div class=w-100><img src=/imageML/15.png alt="unit cell" loading=lazy data-zoomable></div></div></figure></p><p>As we can see, the decision boundary derived using the logistic regression model is very similar to the one obtained with the Support Vector Machine (SVM). Despite the differences in the underlying methods, both approaches successfully capture the separation between the two classes in the feature space. This is especially evident given the high accuracy achieved by the logistic regression model, which indicates that the quadratic form of the decision boundary effectively classifies the data points with a high degree of precision. The similarity between the boundaries reflects how well the quadratic separation hypothesis aligns with the true structure of the data, confirming that this method is capable of capturing the underlying patterns in a way that is comparable to more complex methods like SVMs.</p><p>This method has several advantages:</p><ul><li><strong>Interpretability</strong>: By explicitly constructing the polynomial features, the resulting boundary can be directly interpreted as a specific quadratic equation.</li><li><strong>Simplicity</strong>: This approach avoids the implicit feature mapping of kernel methods, providing a more transparent model.</li><li><strong>Customization</strong>: It allows researchers to leverage their intuition about the dataset to design the feature space, such as adding specific polynomial terms relevant to the problem.</li></ul><p>However, there are limitations to this approach:</p><ul><li><strong>Manual feature design</strong>: It requires some intuition or prior knowledge about the data to determine the appropriate polynomial degree.</li><li><strong>Scalability</strong>: Explicitly generating polynomial features can become computationally expensive for high-degree polynomials or large datasets.</li></ul><h3 id=conclusion-on-nonlinear-classification-and-the-non-edibility-of-plants>Conclusion on nonlinear classification and the non edibility of plants</h3><p>Through the process of classification, the biologist team was able to delineate the area separating the decimated and thriving colonies, which led to the hypothesis that a factor in the center of the zone might be contaminating the plants used by the ants.</p><p>Further investigation revealed the presence of nuclear waste in the central area, likely releasing toxic isotopes into the soil and water, which in turn affected the plants that the ants relied on for food. This contamination, caused by radioactive material, could have impaired the plants&rsquo; ability to produce essential nutrients or even led to the accumulation of harmful substances in the ants&rsquo; food sources.</p><p>By leveraging data-driven techniques, the biologists were able to uncover an underlying environmental hazard.</p><h2 id=conclusion>Conclusion</h2><p>Through these two toy examples, we have explored the fundamental concept of <strong>binary classification</strong>. A variety of machine learning algorithms and techniques have been discussed, including:</p><ul><li><strong>Logistic Regression</strong></li><li><strong>Linear Regression</strong></li><li><strong>Polynomial Classification</strong></li><li><strong>Support Vector Classifiers (SVM)</strong></li><li><strong>Kernel Methods</strong></li><li><strong>Maximimum Likelyhood Estimator</strong></li></ul><p>We also covered essential techniques for improving model performance, such as:</p><ul><li><strong>Feature Engineering</strong></li><li><strong>Convexification of the Loss Function</strong></li><li><strong>Gradient Descent</strong></li><li><strong>Optimization Methods</strong></li></ul><p>Moreover, we highlighted several key concepts that are central to effective machine learning, such as:</p><ul><li><strong>Adaptability</strong></li><li><strong>Specificity</strong></li><li><strong>Robustness</strong></li><li><strong>Flexibility</strong></li></ul><p>The core takeaway from this exploration is not just the mastery of techniques but the underlying philosophy of machine learning. At its core, machine learning is about extracting knowledge from data. The ultimate question it addresses is how we can leverage the computational power of modern systems to uncover patterns, insights, and relationships within vast datasets, enabling us to make more informed decisions and solve complex problems that were once out of reach.</p><div align=center><img src="/imageML/Guillhem Artis.jpg" alt="Author's Photo" width=150 style="border-radius:50%;border:2px solid #1e90ff"><br><strong>Guillhem Artis</strong><br><em>Sorbonne graduate in Machine Learning and Statistics - ENS Mathematics diploma</em><br><a href=https://www.linkedin.com/in/guillhem-artis-6a0618161/ style=color:#1e90ff>LinkedIn</a></div></div><div class=mt-16></div></main></article></div></div><div class=page-footer><footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200"><p class="powered-by text-center">© 2025 OpenVQE was created by Mohammad Haidar. It is licensed under the terms of the MIT License</p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class="powered-by text-center">Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></body></html>